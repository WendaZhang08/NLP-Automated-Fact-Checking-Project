{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "69cb3737-0ffc-4dce-f7cd-f133667067be"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRmNYMfb1g2"
      },
      "source": [
        "## 1.1 Reading and gathering data\n",
        "\n",
        "Using `json` package reading and gathering claims and evidences, then print an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1N-PVoLb1g2",
        "outputId": "44c25abc-6f6a-4a59-bcd5-ec0f17019121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 154\n",
            "Dev evi outside train evi 0\n",
            "Train claim count:  1228\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "\n",
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside += 1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n",
        "print(\"Train claim count: \",len(train_claim_id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCxJ35-b1g3"
      },
      "source": [
        "## 1.2 Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFcmSZmDlfE"
      },
      "source": [
        "### Implementing preprocessing fuctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYaLwC2MDlfF",
        "outputId": "7256f86e-88f2-4ae5-ae1b-65539597cb3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    return lemma if lemma != word else lemmatizer.lemmatize(word, 'n')\n",
        "\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)\n",
        "\n",
        "def remove_non_eng(dictionary):\n",
        "    eng_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if is_pure_english(value):\n",
        "            eng_data[key] = value\n",
        "    return eng_data\n",
        "\n",
        "def contains_climate_keywords(text, keywords):\n",
        "    text = text.lower()\n",
        "    for keyword in keywords:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", text):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def filter_climate_related(dictionary, keywords):\n",
        "    cs_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if contains_climate_keywords(value, keywords):\n",
        "            cs_data[key] = value\n",
        "    return cs_data\n",
        "\n",
        "def text_preprocessing(text, remove_stopwords=False):\n",
        "    words = [lemmatize(w) for w in text.lower().split()]\n",
        "    if remove_stopwords:\n",
        "        words = [w for w in words if w not in stopwords]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqq0xrC_DlfF"
      },
      "source": [
        "### Implementing **Claim data preprocessing** and **Evidence data preprocessing** functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "nsxYPxqgDlfF"
      },
      "outputs": [],
      "source": [
        "# 权威网站 https://www.ucdavis.edu/climate/definitions\n",
        "climate_keywords = [\n",
        "    \"climate\", \"environment\", \"global warming\", \"greenhouse effect\", \"carbon\", \"co2\", \"carbon dioxide\",\n",
        "    \"methane\", \"renewable energy\", \"sustainability\", \"ecology\", \"biodiversity\", \"fossil fuels\",\n",
        "    \"emissions\", \"air quality\", \"ozone\", \"solar energy\", \"wind energy\", \"climate change\", \"climate crisis\",\n",
        "    \"climate adaptation\", \"climate mitigation\", \"ocean\", \"sea levels\", \"ice melting\", \"deforestation\",\n",
        "    \"reforestation\", \"pollution\"\n",
        "]\n",
        "\n",
        "# def filter_evidence_by_train(train_claim_data, evidence_data):\n",
        "\n",
        "#     # Collect all evidence ids in the training set\n",
        "#     train_evidence_ids = set()\n",
        "\n",
        "#     for claim in train_claim_data.values():\n",
        "#         train_evidence_ids.update(claim['evidences'])\n",
        "\n",
        "#     # filter evidence data by the evidence ids in the training set\n",
        "#     filtered_evidence_data = {key: value for key, value in evidence_data.items() if key in train_evidence_ids}\n",
        "\n",
        "#     return filtered_evidence_data\n",
        "\n",
        "def preprocess_claim_data(claim_data, existed_evidences_id=None):\n",
        "    claim_data = remove_non_eng(claim_data)\n",
        "    claim_data_text = []\n",
        "    claim_data_id = []\n",
        "    claim_data_label = []\n",
        "    claim_evidences = []\n",
        "\n",
        "    for key in claim_data.keys():\n",
        "        claim_data[key][\"claim_text\"] = text_preprocessing(claim_data[key][\"claim_text\"])\n",
        "        claim_data_text.append(claim_data[key][\"claim_text\"])\n",
        "        claim_data_id.append(key)\n",
        "\n",
        "        if \"claim_label\" in claim_data[key]:\n",
        "            claim_data_label.append(claim_data[key][\"claim_label\"])\n",
        "        else:\n",
        "            claim_data_label.append(None)\n",
        "\n",
        "        if existed_evidences_id and \"evidences\" in claim_data[key]:\n",
        "            claim_evidences.append([existed_evidences_id[i] for i in claim_data[key][\"evidences\"]])\n",
        "        else:\n",
        "            claim_evidences.append([])\n",
        "\n",
        "    return claim_data_text, claim_data_id, claim_data_label, claim_evidences\n",
        "\n",
        "def preprocess_evi_data(evi_data, climate_keywords, train_claim_data):\n",
        "    evi_data = remove_non_eng(evi_data)\n",
        "    cs_evi_data = filter_climate_related(evi_data, climate_keywords)\n",
        "\n",
        "    # filter evidence data by the evidence ids in the training set\n",
        "    # train_evi_data = filter_evidence_by_train(train_claim_data, cs_evi_data)\n",
        "\n",
        "    for key in cs_evi_data.keys():\n",
        "        cs_evi_data[key] = text_preprocessing(cs_evi_data[key], remove_stopwords=True)\n",
        "\n",
        "    cleaned_evidence_text = list(cs_evi_data.values())\n",
        "    cleaned_evidence_id = list(cs_evi_data.keys())\n",
        "\n",
        "    return cleaned_evidence_text, cleaned_evidence_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8bIR_XDlfF"
      },
      "source": [
        "### Start dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0_ht7aH1DlfF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the claim data, split the data into text, id, label and evidences\n",
        "train_claim_text, train_claim_id, train_claim_label, train_claim_evidences = preprocess_claim_data(train_claim_data)\n",
        "\n",
        "dev_claim_text, dev_claim_id, dev_claim_label, dev_claim_evidences = preprocess_claim_data(dev_claim_data)\n",
        "\n",
        "test_claim_text, test_claim_id, _, _ = preprocess_claim_data(test_claim_data)\n",
        "\n",
        "# Preprocessing the evidence data, split the data into text and id\n",
        "cleaned_evidence_text, cleaned_evidence_id = preprocess_evi_data(evi_data, climate_keywords, train_claim_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train claim count:  1228\n"
          ]
        }
      ],
      "source": [
        "print(\"Train claim count: \",len(train_claim_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3LL9-aEb1g5"
      },
      "source": [
        "## 1.3 Development Set Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4cbdtTb1g5"
      },
      "source": [
        "In this section, we perform the main tasks of the project on the development set:\n",
        "\n",
        "1. **Evidence Retrieval**: For each claim, find the most relevant evidence from the corpus.\n",
        "2. **Claim Classification**: Predict the label for each claim based on the retrieved evidence and the claim's similarity to the training claims.\n",
        "\n",
        "The code uses TF-IDF vectorization and cosine similarity to measure the relevance between claims and evidence, and between development and training claims. The most similar evidence and training claims are used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Creating two vectorizer\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
        "\n",
        "# fit the vectorizer on the evidence data\n",
        "evidence_tfidf_vectorizer.fit(cleaned_evidence_text)\n",
        "\n",
        "# Transform cleaned_evidence_text\n",
        "transformed_evidence = evidence_tfidf_vectorizer.transform(cleaned_evidence_text)\n",
        "\n",
        "# Transform claim data\n",
        "train_claim_tfidf = evidence_tfidf_vectorizer.transform(train_claim_text)\n",
        "dev_claim_tfidf = evidence_tfidf_vectorizer.transform(dev_claim_text)\n",
        "test_claim_tfidf = evidence_tfidf_vectorizer.transform(test_claim_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "58n5gcslrAmV"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between train claims and evidence\n",
        "train_similarity = cosine_similarity(train_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between dev claims and evidence\n",
        "dev_similarity = cosine_similarity(dev_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between test claims and evidence\n",
        "test_similarity = cosine_similarity(test_claim_tfidf, transformed_evidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "fzgteUN7rAmX"
      },
      "outputs": [],
      "source": [
        "def spliting_dataset(similarity, claim_texts, claim_evidences, evidence_texts, top_k=5, neg_ratio=1):\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Based on the similarity matrix, find the top k most similar evidence for each claim\n",
        "    for i in range(similarity.shape[0]):\n",
        "\n",
        "        claim_text = claim_texts[i]\n",
        "\n",
        "        # Find the top k most similar evidence\n",
        "        top_evidences = np.argsort(-similarity[i])[:top_k]\n",
        "\n",
        "        # Add the top k most similar evidence to the dataset, label as 1\n",
        "        for evidence_index in top_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(1)\n",
        "\n",
        "        # If the claim has evidences, add the evidence to the dataset, label as 1\n",
        "        if claim_evidences is not None:\n",
        "            for evidence_index in claim_evidences[i]:\n",
        "                evidence_text = evidence_texts[evidence_index]\n",
        "                dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "                labels.append(1)\n",
        "\n",
        "        # Randomly sample negative samples, label as 0\n",
        "        neg_samples_num = int(neg_ratio * len(top_evidences))\n",
        "\n",
        "        # Randomly sample negative samples that are not in the top k most similar evidence\n",
        "        neg_evidences = np.random.choice(\n",
        "            [j for j in range(similarity.shape[1]) if j not in top_evidences],\n",
        "            neg_samples_num\n",
        "        )\n",
        "\n",
        "        # Add the negative samples to the dataset\n",
        "        for evidence_index in neg_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(0)\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1yZ4FGlrrAmX"
      },
      "outputs": [],
      "source": [
        "train_dataset, train_dataset_labels = spliting_dataset(\n",
        "    train_similarity, train_claim_text, train_claim_evidences, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")\n",
        "dev_dataset, dev_dataset_labels = spliting_dataset(\n",
        "    dev_similarity, dev_claim_text, None, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")\n",
        "test_dataset, test_dataset_labels = spliting_dataset(\n",
        "    test_similarity, test_claim_text, None, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZxkXsHUgb1g7"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset labels to numpy array\n",
        "train_label_array = np.array(train_dataset_labels)\n",
        "dev_label_array = np.array(dev_dataset_labels)\n",
        "test_label_array = np.array(test_dataset_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "a9pkJPuab1g8"
      },
      "outputs": [],
      "source": [
        "# need to install\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ICh3nDmFb1g8"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  # 0 is padding token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "W2Sj6qFub1g8"
      },
      "outputs": [],
      "source": [
        "# Convert the text data to sequence\n",
        "train_sequence = tokenizer.texts_to_sequences(train_dataset)\n",
        "dev_sequence = tokenizer.texts_to_sequences(dev_dataset)\n",
        "test_sequence = tokenizer.texts_to_sequences(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KCi9XwMPb1g8"
      },
      "outputs": [],
      "source": [
        "longest_train_sequence = 0\n",
        "for i in train_sequence:\n",
        "    longest_train_sequence = max(longest_train_sequence, len(i))\n",
        "\n",
        "longest_dev_sequence = 0\n",
        "for i in dev_sequence:\n",
        "    longest_dev_sequence = max(longest_dev_sequence, len(i))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "WQyglIFSb1hA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_length = max(longest_train_sequence, longest_dev_sequence) + 5\n",
        "\n",
        "padded_train_sequence = pad_sequences(train_sequence, maxlen=padding_length, padding='post')\n",
        "padded_dev_sequence = pad_sequences(dev_sequence, maxlen=padding_length, padding='post')\n",
        "padded_test_sequence = pad_sequences(test_sequence, maxlen=padding_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG_0JSzdb1hA",
        "outputId": "76b02cfc-1689-480e-d585-60c369ceda48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"retrieval_cls_lstm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 162, 200)          2893000   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 162, 200)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 162, 800)         1923200   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 800)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 400)               320400    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 401       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,137,001\n",
            "Trainable params: 5,137,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# from workshop\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "embedding_dim = 200\n",
        "hidden_dim = 400\n",
        "\n",
        "#model definition\n",
        "# feedforward network (MLP)\n",
        "model = Sequential(name=\"retrieval_cls_lstm\")\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=padding_length, embeddings_regularizer=l2(0.02)))\n",
        "\n",
        "model.add(layers.Dropout(0.6))\n",
        "# model.add(LSTM(hidden_dim, return_sequences=True, dropout=0.1))\n",
        "# model.add(LSTM(hidden_dim, dropout=0.1))\n",
        "\n",
        "model.add(layers.Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.6, kernel_regularizer=l2(0.02), recurrent_regularizer=l2(0.02))))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(layers.Dense(hidden_dim, activation='tanh', kernel_regularizer=l2(0.02), bias_regularizer=l2(0.02)))\n",
        "model.add(layers.Dropout(0.6))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.Recall()])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "decay_steps = 3000\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    learning_rate, decay_steps\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2RL1LOb1hA",
        "outputId": "ff3742dd-d3bd-40e0-888a-e6d846e76f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "125/192 [==================>...........] - ETA: 4:12 - loss: 60.4056"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
            "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
            "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
            "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(padded_train_sequence,train_label_array,epochs=15,validation_data=(padded_dev_sequence, dev_label_array),verbose=True,batch_size=64,callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUG6D3SPrAmZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: retrieval_cls_lstm\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: retrieval_cls_lstm\\assets\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('retrieval_cls_lstm')\n",
        "\n",
        "# Load the model\n",
        "# model = tf.keras.models.load_model('retrieval_cls_lstm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49/49 [==============================] - 1s 20ms/step\n",
            "48/48 [==============================] - 1s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "# Start prediction\n",
        "\n",
        "dev_predictions = model.predict(padded_dev_sequence, batch_size=64)\n",
        "test_predictions = model.predict(padded_test_sequence, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidences_retrieval(claim_evidence_scores, top_k):\n",
        "    \n",
        "    top_evidence_indices = []\n",
        "    \n",
        "    for scores in claim_evidence_scores:\n",
        "        sorted_indices = np.argsort(scores)[::-1]\n",
        "        top_indices = sorted_indices[:top_k]\n",
        "        top_evidence_indices.append(top_indices)\n",
        "    \n",
        "    return top_evidence_indices\n",
        "\n",
        "\n",
        "select_evidence_k = 6\n",
        "dev_top_evidence_indices = evidences_retrieval(dev_predictions, select_evidence_k)\n",
        "test_top_evidence_indices = evidences_retrieval(test_predictions, select_evidence_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'claim-752': {'claim_text': '[South Australia] has the most expensive electricity in the world.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-375': {'claim_text': 'when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod\\xaduces 1.3 per cent of this 3 per cent, then no amount of emissions reductio\\xadn here will have any effect on global climate.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1266': {'claim_text': 'This means that the world is now 1C warmer than it was in pre-industrial times', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-871': {'claim_text': '“As it happens, Zika may also be a good model of the second worrying effect — disease mutation.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2164': {'claim_text': 'Greenland has only lost a tiny fraction of its ice mass', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1607': {'claim_text': \"CO2 limits won't cool the planet.\", 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-761': {'claim_text': '[Riebesell] is a world authority on the topic and has typically communicated cautiously about the effects of acidification.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1718': {'claim_text': 'The actual data show high northern latitudes are warmer today than in 1940.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1273': {'claim_text': 'The rapid changes in the climate may have profound consequences for humans and other species… Severe drought caused food shortages for millions of people in Ethiopia, with a lack of rainfall resulting in “intense and widespread” forest fires in Indonesia that belched out a vast quantity of greenhouse gas', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1786': {'claim_text': 'CFCs contribute to global waerming at a small level.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-2796': {'claim_text': 'Had he used the currently accepted value of approximately 3°C warming for a doubling of atmospheric CO2, Hansen would have correctly projected the ensuing global warming.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2580': {'claim_text': 'Volcanoes emit around 0.3 billion tonnes of CO2 per year.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-1219': {'claim_text': 'Global warming is driving major melting on the surface of Greenland’s glaciers and is speeding up their travel into the sea.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-75': {'claim_text': 'The science is clear, climate change is making extreme weather events, including tornadoes, worse.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2813': {'claim_text': 'In fact, the authors go on to estimate climate sensitivity from their findings, calculate a value between 2.3 to 4.1°C.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2335': {'claim_text': 'Satellite measurements of infrared spectra over the past 40 years observe less energy escaping to space at the wavelengths associated with CO2.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-161': {'claim_text': 'Extreme melting and changes to the climate like this has released pressure on to the continent, allowing the ground to rise up.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2243': {'claim_text': 'The CO2 amplifies the warming and mixes through the atmosphere, spreading warming throughout the planet.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1256': {'claim_text': '‘Next year or the year after, the Arctic will be free of ice’', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-506': {'claim_text': 'More than 100 climate models over the past 30 years did not predict what actually happened because it was assumed carbon dioxide had the pivotal role in driving climate change and that the effects of clouds, back-radiation and the sun were trivial.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-369': {'claim_text': 'By 2050 there’s a scientific consensus that we reached the tipping point for ice sheets in Greenland and the West Antarctic', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2184': {'claim_text': 'We have had ice ages and warmer periods when alligators were found in Spitzbergen.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1057': {'claim_text': '“Moreover, the ocean already contains so-called oxygen minimum zones, generally found in the middle depths.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-104': {'claim_text': 'Increases in atmospheric CO2 followed increases in temperature.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1975': {'claim_text': 'The claim that 97 percent of scientists believe humans are causing climate change has been debunked by the \"head\" of the United Nations Intergovernmental Panel on Climate Change.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-139': {'claim_text': 'And in January, one out of five British children told pollsters they were having nightmares about climate change.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2062': {'claim_text': 'Global warming is increasing the magnitude and frequency of droughts and floods.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1160': {'claim_text': 'With more CO2 in the atmosphere, the challenge [feeding 2.5 billion more people] can and will be met.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2679': {'claim_text': '\"The solubility of carbon dioxide in water is listed in the Handbook of Chemistry and Physics as a declining function of temperature. ...', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2662': {'claim_text': '\"...there is the contention by Wolfgang Knorr of the Department of Earth Sciences at the University of Bristol in England that carbon dioxide levels in the atmosphere are about where they were 160 years ago.\" (as quoted by Ken Ward Jr.)', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1490': {'claim_text': 'the concentration of carbon dioxide in Earth’s atmosphere has climbed to a level last seen more than 3 million years ago — before humans even appeared on the rocky ball we call home.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2768': {'claim_text': 'The long term trend from albedo is that of cooling.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2168': {'claim_text': 'IPCC graph showing accelerating trends is misleading', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-785': {'claim_text': 'In many other cases, though — hurricanes, for example — the linkage to global warming for particular trends is uncertain or disputed.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2426': {'claim_text': '\"Twentieth century global warming did not start until  1910.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1292': {'claim_text': 'Any reasonable person can recognize both positives and negatives among the policy proposals of both Tories and Labour.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-993': {'claim_text': 'While there has been a mean rise of a little more than 3mm per year worldwide since the 1990s, in the last decade, the NOAA Virginia Key tide gauge just south of Miami Beach has measured a 9mm rise annually.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2593': {'claim_text': '[T]he study indicates “Greenland’s ice may be less susceptible to the massive meltdown predicted by computer models of climate change, the main author ... said in an interview. ...', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1567': {'claim_text': 'IPCC overestimate temperature rise.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1834': {'claim_text': 'A 14 August 1912 article from a New Zealand newspaper contained a brief story about how burning coal might produce future warming by adding carbon dioxide to the atmosphere.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-856': {'claim_text': 'as the pathbreaking work by Rosamond Naylor and David Battisti has shown, the tropics are already too hot to efficiently grow grain, and those places where grain is produced today are already at optimal growing temperature — which means even a small warming will push them down the slope of declining productivity.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-540': {'claim_text': 'The heatwave we now have in Europe is not something that was expected with just 1C of warming', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-757': {'claim_text': 'The grasslands, crops, forests and territorial waters of Australia absorb more carbon dioxide than Australia emits.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-1407': {'claim_text': '“In their award winning book, ‘Taken By Storm’ (2007), Canadian researchers Christopher Essex and Ross McKitrick explain: ‘Temperature is not an amount of something [like height or weight].', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-3070': {'claim_text': 'So this is a government which is proposing to put at risk our manufacturing industry, to penalise struggling families, to make a tough situation worse for millions of households right around Australia.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1745': {'claim_text': 'The IPCC simply updated their temperature history graphs to show the best data available at the time.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1515': {'claim_text': 'no one really knows if last year 2016 was a global temperature record.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1519': {'claim_text': 'The Earth’s climate is changing in response to increasing concentrations of greenhouse gases (GHGs) and particulate matter in the atmosphere, largely as the result of human activities.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-3069': {'claim_text': 'Continued greenhouse gas emissions at or above current rates would cause further warming and induce many changes in the global climate system during the 21st century that would very likely be larger than those observed during the 20th century.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-677': {'claim_text': '“Which is to say that these beans will be eaten by cows, and the cows will convert the beans to meat, and the humans will eat the meat.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-765': {'claim_text': 'The broader term covers changes beyond warmer temperatures, such as shifting rainfall patterns.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2275': {'claim_text': '[…] Constant 24-7  media coverage of every significant storm worldwide just makes it seem  that way.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1113': {'claim_text': 'This means the jet stream meanders more, with big loops bringing warm air to the frozen north and cold air into warmer southern climes.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2611': {'claim_text': 'The final amount of extra CO2 that remains in the atmosphere stays there on a time scale of centuries.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2060': {'claim_text': 'Global warming is causing more hurricanes and stronger hurricanes.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2326': {'claim_text': '\"In 2007, the Northern Hemisphere reached a record low in ice coverage and the Northwest Passage was opened.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1087': {'claim_text': 'The particular signature of warming in 2016 was also revealing in another way, Overpeck said, noting that the stratosphere… saw record cold temperatures last year', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2867': {'claim_text': \"Skeptics who oppose scientific findings that threaten their world view are far closer to Galileo's belief-based critics in the Catholic Church.\", 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2300': {'claim_text': '\"Austria is today seeing its earliest snowfall in history with 30 to 40 centimetres already predicted in the mountains.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2250': {'claim_text': \"Some global warming 'skeptics' argue that the Earth's climate sensitivity is so low that a doubling of atmospheric CO2 will result in a surface temperature change on the order of 1°C or less, and that therefore global warming is nothing to worry about.\", 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-2429': {'claim_text': 'With 32 years of  rapidly increasing global temperatures and only a minor increase in  global CO2 emissions, followed by 33 years of slowly cooling global  temperatures with rapid increases in global CO2 emissions, it was  deceitful for the IPCC to make any claim that CO2 emissions were  primarily responsible for observed 20th century global warming.\"', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-3051': {'claim_text': 'Once natural influences, in particular the impact of El Niño and La Niña, are removed from the recent termperature record, there is no evidence of a significant change in the human contribution to climate change.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-1549': {'claim_text': \"There's no empirical evidence for climate change.\", 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-261': {'claim_text': 'in 1995 one scientist at the IPCC – Jonathan Overpeck – wrote an email to a colleague claiming ‘we have to get rid of the Medieval Warm Period.’', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2230': {'claim_text': '[Ice] is expanding in much of Antarctica, contrary to the widespread public belief that global warming is melting the continental ice cap.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2579': {'claim_text': 'Over the past 250 years, humans have added just one part of CO2 in 10,000 to the atmosphere.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1416': {'claim_text': 'In recent decades this warming has been accompanied by a constant rise in the sea level and, it would appear, by an increase of extreme weather events, even if a scientifically determinable cause cannot be assigned to each particular phenomenon.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2497': {'claim_text': \"Without the forests' humidity, previously moisture-laden winds blew dry.\", 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-811': {'claim_text': 'But [climate scientists] say that aspects of the case of Hurricane Harvey—and the recent history of tropical cyclones worldwide—suggest global warming is making a bad situation worse.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1896': {'claim_text': 'Greg Hunt CSIRO research shows carbon emissions can be reduced by 20 per cent over 40 years using nature, soils and trees.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2819': {'claim_text': 'More importantly, the OISM list only contains 39 scientists who specialise in climate science.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2643': {'claim_text': 'The \"decline\" refers to a decline in northern tree-rings, not global temperature, and is openly discussed in papers and the IPCC reports.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1775': {'claim_text': 'Venus very likely underwent a runaway or ‘moist’ greenhouse phase earlier in its history, and today is kept hot by a dense CO2 atmosphere.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-316': {'claim_text': 'the models predicted seven times as much warming as has been observed', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-896': {'claim_text': 'According to the WHO, exposure to particulate matter increases the risk of acute lower respiratory infection, chronic obstructive pulmonary disease, heart disease, stroke and lung cancer.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-331': {'claim_text': 'But each serial adjustment has tended to make the early years colder, which increases the warming trend.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2574': {'claim_text': 'On a world scale coral reefs are in decline.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-342': {'claim_text': 'While members of the media may nod along to such claims [about changes in weather extremes], the evidence paints a different story', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2034': {'claim_text': 'Latest IPCC Reports (AR5) have shown global mean temperature forecasts from the 2005 IPCC report exceeded actual readings.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-578': {'claim_text': '‘If we do nothing to reduce our greenhouse gas emissions, the kind of extreme heat we saw this past summer will be the norm when my young son is a grown man.’', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-976': {'claim_text': 'The gas builds up in the soil, forming mounds called ‘pingoes.’', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1097': {'claim_text': '“[…]The impact on calcification, metabolism, growth, fertility and survival of calcifying marine species when pH is lowered up to 0.3 units […] is beneficial, not damaging.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-609': {'claim_text': 'That’s because as Antarctica’s mass shrinks, the ice sheet’s gravitational pull on the ocean relaxes somewhat, and the seas travel back across the globe to pile up far away — with U.S. coasts being one prime destination.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-173': {'claim_text': 'If there were [carbon emissions], we could not see because most carbon is black.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1222': {'claim_text': 'The melting Greenland ice sheet is already a major contributor to rising sea level and if it was eventually lost entirely, the oceans would rise by six metres around the world, flooding many of the world’s largest cities.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2441': {'claim_text': 'Furthermore, it is physically incorrect to state that the planet is simply \"recovering\" from the Little Ice Age.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-756': {'claim_text': 'Australia’s signed a suicide note [with the Paris Accord] yet didn’t seem to notice that China, India, Indonesia and the US did not commit to reducing their large carbon dioxide emissions.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2577': {'claim_text': '\"Three recent articles give us reason to question the alarmists’ claims  that coral reefs are in deep trouble due to the buildup of greenhouse  gases.\" (World Climate Report)', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2890': {'claim_text': 'Heat is continuing to build up in the subsurface ocean.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2478': {'claim_text': '(Kerr 2007) points out that the sunlight-reflecting haze that cools much of the planet seems to have thinned over the past decade or so.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2399': {'claim_text': 'While the greenhouse effect is a natural occurence, too much warming has severe negative impacts on agriculture, health and environment.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-3091': {'claim_text': 'Global Warming history completely coincides with the history of artificial satellites and the use of microwave frequencies from outer space.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-141': {'claim_text': 'Greenpeace didn’t save the whales, switching from whale oil to petroleum and palm oil did', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1933': {'claim_text': 'Newt Gingrich \"teamed with Nancy Pelosi and Al Gore on global warming.\"', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1689': {'claim_text': 'Evidence is building that net cloud feedback is likely positive and unlikely to be strongly negative.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-443': {'claim_text': '“Today climate scientists are obsessed with the level of carbon dioxide in the atmosphere, a very very small part of the overall picture.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-2037': {'claim_text': 'The rate of renewable energy installations in the EU in 2018 was less than half the maximum level achieved in 2010.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1734': {'claim_text': 'Jim Hansen had several possible scenarios; his mid-level scenario B was right.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2093': {'claim_text': 'Through its impacts on the climate, CO2 presents a danger to public health and welfare, and thus qualifies as an air pollutant', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1400': {'claim_text': 'The late 1970s marked the end of a 30-year cooling trend.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1638': {'claim_text': 'Worry about global warming impacts in the next 100 years, not an ice age in over 10,000 years.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-3075': {'claim_text': \"Multiple lines of evidence indicate Greenland's ice loss is accelerating and will contribute sea level rise in the order of metres over the next few centuries.\", 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-38': {'claim_text': 'Scientists studying Antarctica sea ice warn a rise in accumulation could spark the next ice age.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1643': {'claim_text': \"Al Gore's book is quite accurate, and far more accurate than contrarian books.\", 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1259': {'claim_text': 'While the north-east, midwest and upper great plains have experienced a 30% increase in heavy rainfall episodes – considered once-in-every-five year downpours – parts of the west, particularly California, have been parched by drought.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1605': {'claim_text': 'Scientists retracted claim that sea levels are rising.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1711': {'claim_text': \"Global warming' and 'climate change' mean different things and have both been used for decades.\", 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2236': {'claim_text': 'The majority of peer reviewed research at the time predicted warming due to increasing CO2.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1040': {'claim_text': 'While evidence that the earth’s orbital variations impact radiation levels and thus global temperatures does not of course mean that man is not in some way impacting the climate, studies like these highlight that the role man plays on the planet is dwarfed by natural phenomena utterly out of our control.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-392': {'claim_text': 'a study that totally debunks the whole concept of man-made Global Warming', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-368': {'claim_text': 'The most recent IPCC report lays out a future if we limit global heating to 1.5°C instead of the Paris Agreement’s 2°C.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-559': {'claim_text': 'And there is a lot of evidence that climate change is diminishing biodiversity, which can be seen in these alpine meadows as well.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2583': {'claim_text': '\"Each unit of CO2 you put into the atmosphere has less and less of a warming impact.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2609': {'claim_text': 'Individual carbon dioxide molecules have a short life time of around 5 years in the atmosphere.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-492': {'claim_text': 'But like most claims regarding global warming, the real effect is small, probably temporary, and most likely due to natural weather patterns', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-1420': {'claim_text': 'Concentrated in the atmosphere, these gases do not allow the warmth of the sun’s rays reflected by the earth to be dispersed in space.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1089': {'claim_text': 'Last year’s warmth was manifested across the planet, from the warm tropical ocean waters off the coast of northeastern Australia, where the Great Barrier Reef experienced its worst coral bleaching event on record and large scale coral death, to the Arctic, where sea ice hit regular monthly record lows and overall temperatures were also the warmest on record, at least from January through September 2016.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1467': {'claim_text': 'The amount of carbon dioxide absorbed by the upper layer of the oceans is increasing by about 2 billion tons per year.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-444': {'claim_text': 'Carbon dioxide is a trace gas.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-803': {'claim_text': 'The team of climate scientists notes that in failing to predict the warming ‘hiatus’ in the beginning of the 21st century, the Intergovernmental Panel on Climate Change (IPCC) models overestimated temperature increases…', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-1668': {'claim_text': 'Thick arctic sea ice is in rapid retreat.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-742': {'claim_text': 'Global human emissions are only 3 per cent of total annual emissions.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-846': {'claim_text': 'In other words, we have, trapped in Arctic permafrost, twice as much carbon as is currently wrecking the atmosphere of the planet, all of it scheduled to be released at a date that keeps getting moved up, partially in the form of a gas that multiplies its warming power 86 times over.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2119': {'claim_text': 'Lindzen and Choi find low climate sensitivity', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1167': {'claim_text': 'Lake-bottom sediments in Florida tell us that recent major hurricane activity in the Gulf of Mexico has been less frequent than in centuries past.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-623': {'claim_text': 'The noted oceanographer Walter Munk referred to sea-level rise as an “enigma”', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2882': {'claim_text': 'Trenberth\\'s views are clarified in the paper \"An imperative for climate change planning: tracking Earth\\'s global energy\".', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1698': {'claim_text': 'Around 97% of climate experts agree that humans are causing global warming.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-181': {'claim_text': 'Reefs need carbon dioxide; it’s their basic food.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-281': {'claim_text': 'In an interview with the BBC after the scandal broke, Dr Jones admitted there had been no statistically significant global warming since 1995', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2809': {'claim_text': 'The motions of the massive oceans where heat is moved between deep layers and the surface provides variability on time scales from years to centuries.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-1928': {'claim_text': 'NASA Finds Antarctica is Gaining Ice,', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-2787': {'claim_text': \"Hansen's 1988 results are evidence that the actual climate sensitivity is about 3°C for a doubling of atmospheric CO2.\", 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-478': {'claim_text': 'Protecting and restoring forests would reduce 18% of emissions by 2030', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-988': {'claim_text': 'The unlikely scenarios are now, all of a sudden, becoming more probable than they once were thought to be,’ says Sweet.”', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-266': {'claim_text': 'The IPCC no longer includes the ‘Hockey stick’ chart in its reports.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-2282': {'claim_text': 'While there are isolated cases of growing glaciers, the overwhelming trend in glaciers worldwide is retreat.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2895': {'claim_text': 'The latest  measurements involve the use of satellite gravimetry, estimating  the mass of terrain beneath by detecting slight changes in gravity as a  satellite passes overhead.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-349': {'claim_text': 'Nor is there evidence of an increase in floods globally.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-2101': {'claim_text': 'Clouds provide negative feedback', 'claim_label': 'DISPUTED', 'evidences': ['evidence-4241']}, 'claim-897': {'claim_text': 'The report […] found that the United States was one of the most pollution-free nations in the world.”', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-3063': {'claim_text': \"'Another global warming myth comes crashing down.\", 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-386': {'claim_text': 'Renew\\xadables such as wind turbines are environmentally disastrous because they pollute a huge land area, slice and dice birds and bats, kill insects that are bird food, create health problems for humans who live within kilometres of them, leave toxins around the turbine site and despoil the landscape.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-2691': {'claim_text': \"There are a myriad of other radiative forcings that affect the planet's energy imbalance.\", 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-530': {'claim_text': 'South Australia is winning: it has the most unreliable grid in the world outside Africa and the most expensive electricity.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2979': {'claim_text': 'Water vapor helps trap heat, and is a far the strongest of the major greenhouse gases, contributing 36–72 percent of the greenhouse effect.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-665': {'claim_text': 'many scientists were surprised when other researchers subsequently found that ringed and bearded seals (the primary prey of polar bears) north of the Bering Strait especially thrived with a longer open-water season, which is particularly conducive to fishing', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-199': {'claim_text': 'In the past, warming has never been a threat to life on Earth.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-490': {'claim_text': 'IPCC report warning last week the world is “nowhere near on track” to meet its Paris commitments', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}, 'claim-2400': {'claim_text': '\\'To suddenly label CO2 as a \"pollutant\" is a disservice to a gas that has played an enormous role in the development and sustainability of all life on this wonderful Earth.', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-204': {'claim_text': 'after a natural orbitally driven warming, atmospheric carbon dioxide content increases 800 years later', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-1426': {'claim_text': 'Many of the world’s coral reefs are already barren or in a state of constant decline.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-4241']}, 'claim-698': {'claim_text': 'A recent study led by Lawrence Livermore National Laboratory climate scientist Ben Santer found that while the models ran hot, the ‘overestimation’ was ‘partly due to systematic deficiencies in some of the post-2000 external forcings used in the model simulations.’', 'claim_label': 'REFUTES', 'evidences': ['evidence-4241']}, 'claim-1021': {'claim_text': 'The corals may save themselves, as many other creatures are attempting to do, by moving toward the poles as the Earth warms, establishing new reefs in cooler water.”', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-4241']}}\n"
          ]
        }
      ],
      "source": [
        "# Update the dev JSON file\n",
        "with open('data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)\n",
        "\n",
        "for claim_id, evidence_indices in zip(dev_claim_id, dev_top_evidence_indices):\n",
        "    top_evidence_ids = [cleaned_evidence_id[idx] for idx in evidence_indices]\n",
        "    dev_claims[claim_id]['evidences'] = top_evidence_ids\n",
        "\n",
        "with open('data/dev-claims.json', 'w') as f:\n",
        "    json.dump(dev_claims, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the test JSON file\n",
        "with open('data/test-claims-unlabelled.json', 'r') as f:\n",
        "    test_claims = json.load(f)\n",
        "\n",
        "for claim_id, evidence_indices in zip(test_claim_id, test_top_evidence_indices):\n",
        "    top_evidence_ids = [cleaned_evidence_id[idx] for idx in evidence_indices]\n",
        "    test_claims[claim_id]['evidences'] = top_evidence_ids\n",
        "\n",
        "with open('data/test-claims-unlabelled.json', 'w') as f:\n",
        "    json.dump(test_claims, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "20d96abf-cae9-4441-f21e-238786b63b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F): 0.0\n",
            "Claim Classification Accuracy (A): 0.38961038961038963\n",
            "Harmonic Mean of F and A: 0.0\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "output_str = output.decode('utf-8')\n",
        "\n",
        "# Split the output into lines\n",
        "output_lines = output_str.strip().split('\\n')\n",
        "\n",
        "# Format the output\n",
        "formatted_lines = []\n",
        "for line in output_lines:\n",
        "    metric, value = line.split('=')\n",
        "    metric = metric.strip()\n",
        "    value = value.strip()\n",
        "    formatted_line = f\"{metric}: {value}\"\n",
        "    formatted_lines.append(formatted_line)\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "formatted_output = '\\n'.join(formatted_lines)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
