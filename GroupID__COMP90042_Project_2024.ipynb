{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "8cdd074d-60b9-493a-9d8e-4f9a18b246fa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import operator\n",
        "from statistics import mean\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Reading and gathering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 163\n",
            "Dev evi outside train evi 328\n"
          ]
        }
      ],
      "source": [
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside += 1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Remove unnecessary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\steph\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['evidence-0', 'evidence-1', 'evidence-2', 'evidence-3', 'evidence-4', 'evidence-5', 'evidence-6', 'evidence-7', 'evidence-8', 'evidence-9']\n"
          ]
        }
      ],
      "source": [
        "print(full_evidence_id[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# remove non-english words\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1193821\n",
            "1208827\n",
            "('evidence-0', 'John Bennet Lawes, English entrepreneur and agricultural scientist')\n"
          ]
        }
      ],
      "source": [
        "print(len(set(full_evidence_text)))\n",
        "print(len(full_evidence_text))\n",
        "print(next(iter(evi_data.items())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1114577\n",
            "1208827\n"
          ]
        }
      ],
      "source": [
        "# find the keys of a dictionary given a set of values\n",
        "def remove_non_eng(dictionary):\n",
        "    \n",
        "    # create a dictionary with values as keys and keys as values\n",
        "    eng_evi = {}\n",
        "\n",
        "    # populate the dictionary\n",
        "    for key, value in dictionary.items():\n",
        "        if is_pure_english(value):\n",
        "            eng_evi[key] = value\n",
        "    \n",
        "    return eng_evi\n",
        "\n",
        "# find the keys of the english evidence\n",
        "eng_evi = remove_non_eng(evi_data)\n",
        "print(len(eng_evi))\n",
        "print(len(evi_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "english_evidence = list(eng_evi.values())\n",
        "english_evidence_id = list(eng_evi.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re  # 引入正则表达式\n",
        "\n",
        "# 权威网站 https://www.ucdavis.edu/climate/definitions\n",
        "climate_keywords = [\n",
        "    \"climate\", \"environment\", \"global warming\", \"greenhouse effect\", \"carbon\", \"co2\", \"carbon dioxide\", \n",
        "    \"methane\", \"renewable energy\", \"sustainability\", \"ecology\", \"biodiversity\", \n",
        "    \"fossil fuels\", \"emissions\", \"air quality\", \"ozone\", \"solar energy\", \"wind energy\", \n",
        "    \"climate change\", \"climate crisis\", \"climate adaptation\", \"climate mitigation\", \n",
        "    \"ocean\", \"sea levels\", \"ice melting\", \"deforestation\", \"reforestation\", \"pollution\"\n",
        "]\n",
        "\n",
        "\n",
        "# 支持模糊匹配的函数\n",
        "def contains_climate_keywords(text, keywords):\n",
        "    text = text.lower()  # 将文本转为小写以忽略大小写差异\n",
        "    for keyword in keywords:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", text):  # 使用正则表达式匹配整个词\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 移除与气候科学无关的项\n",
        "def filter_climate_related(dictionary, keywords):\n",
        "    cs_evi = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if contains_climate_keywords(value, keywords):\n",
        "            cs_evi[key] = value\n",
        "    return cs_evi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "cs_evi = filter_climate_related(eng_evi, climate_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1114577\n",
            "19324\n"
          ]
        }
      ],
      "source": [
        "print(len(eng_evi))\n",
        "print(len(cs_evi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dev remove stop word会好一点， test不remove反而好\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "\n",
        "# claim_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "\n",
        "\n",
        "evidence_tfidf_vectorizer.fit(train_claim_text+english_evidence) #english_evidence#\n",
        "train_claim_emb_list = claim_tfidf_vectorizer.fit_transform(train_claim_text)\n",
        "\n",
        "\n",
        "full_evi_emb_list = evidence_tfidf_vectorizer.transform(english_evidence) #english_evidence#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1114577, 479945)\n",
            "  (0, 379647)\t0.34129216009737795\n",
            "  (0, 243628)\t0.5418404201055191\n",
            "  (0, 214354)\t0.23690875712907947\n",
            "  (0, 138633)\t0.3635156297255776\n",
            "  (0, 137916)\t0.2362850989994899\n",
            "  (0, 53862)\t0.48806363421951915\n",
            "  (0, 19522)\t0.32805866743609824\n"
          ]
        }
      ],
      "source": [
        "print(full_evi_emb_list.shape)\n",
        "print(full_evi_emb_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "khk_kR5dnDh4"
      },
      "outputs": [],
      "source": [
        "evi_k=2\n",
        "claim_k=1\n",
        "\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    test_out_temp = json.load(input_file)\n",
        "\n",
        "for claim_id,claim_value in test_out_temp.items():\n",
        "    # Task1\n",
        "    # 把test claim转化成vector\n",
        "\n",
        "    test_claim_emb = evidence_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "    evi_sim_dict = {}\n",
        "    # 计算出test claim和所有evidence的相似度\n",
        "    sim = cosine_similarity(test_claim_emb, full_evi_emb_list)[0]\n",
        "\n",
        "    for i in range(len(sim)):\n",
        "        evi_sim_dict[english_evidence_id[i]] = sim[i] #full_evidence_id[i]\n",
        "\n",
        "    # 对evidence根据和claim的相似度排序\n",
        "    s_sim = [(k, v) for k, v in sorted(evi_sim_dict.items(), key=lambda item: item[1],reverse=True)][:evi_k]\n",
        "    sel_sim = [k for k,v in s_sim]\n",
        "    # 把最相似的前k个evidence的id写入到test claim的evidence list\n",
        "    test_out_temp[claim_id][\"evidences\"] = sel_sim\n",
        "\n",
        "    # Task2\n",
        "    # 把test claim转化成vector 注意我这两问用了不同的vectorizer，因为有不同的预处理步骤，和应用数据目标\n",
        "    test_claim_emb = claim_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "\n",
        "    # 计算出test claim和所有train claim的相似度\n",
        "    claim_sim_dict = {}\n",
        "    claim_sim = cosine_similarity(test_claim_emb, train_claim_emb_list)[0]\n",
        "    for i in range(len(claim_sim)):\n",
        "        claim_sim_dict[train_claim_id[i]] = claim_sim[i]\n",
        "\n",
        "    # 取最相似的k个train claim\n",
        "    most_sim_claims = [(k, v) for k, v in sorted(claim_sim_dict.items(), key=lambda item: item[1],reverse=True)]\n",
        "    # 我这里用的是k=1只考虑最相似的那一个，label拿出来\n",
        "    most_sim_claim = max(most_sim_claims, key=operator.itemgetter(1))[0]\n",
        "\n",
        "    test_out_temp[claim_id][\"claim_label\"] = train_claim_data[most_sim_claim][\"claim_label\"]\n",
        "\n",
        "\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"data/dev_predict.json\", \"w\") as outfile:\n",
        "    json.dump(test_out_temp, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n",
            "1114577\n",
            "(1114577, 479945)\n",
            "0.10499500544162468\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(full_evidence_id))\n",
        "print(len(english_evidence_id))\n",
        "print(full_evi_emb_list.shape)\n",
        "#没写进去 可能是sim没算出来#\n",
        "print(evi_sim_dict[\"evidence-1\"])\n",
        "#(1114577, 479945)\n",
        "#0.0 但是text用english但id用full就会是0.1057....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "d380c94d-3adf-4f7e-a751-cf737fe5a546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F): 0.10510204081632651\n",
            "Claim Classification Accuracy (A): 0.474025974025974\n",
            "Harmonic Mean of F and A: 0.17205555936935077\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "output_str = output.decode('utf-8')\n",
        "\n",
        "# Split the output into lines\n",
        "output_lines = output_str.strip().split('\\n')\n",
        "\n",
        "# Format the output\n",
        "formatted_lines = []\n",
        "for line in output_lines:\n",
        "    metric, value = line.split('=')\n",
        "    metric = metric.strip()\n",
        "    value = value.strip()\n",
        "    formatted_line = f\"{metric}: {value}\"\n",
        "    formatted_lines.append(formatted_line)\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "formatted_output = '\\n'.join(formatted_lines)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
