{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "8cdd074d-60b9-493a-9d8e-4f9a18b246fa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import operator\n",
        "from statistics import mean\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIEqDDT78q39",
        "outputId": "b01aad22-1fb8-4b74-a2be-edb82760fd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 163\n",
            "Dev evi outside train evi 328\n"
          ]
        }
      ],
      "source": [
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside +=1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove unnecessary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['evidence-0', 'evidence-1', 'evidence-2', 'evidence-3', 'evidence-4', 'evidence-5', 'evidence-6', 'evidence-7', 'evidence-8', 'evidence-9']\n"
          ]
        }
      ],
      "source": [
        "print(full_evidence_id[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word count:  1208827\n",
            "english word count:  1114577\n",
            "word deleted:  94250\n",
            "word deleted percentage:  0.07796814597953222\n",
            "['John Bennet Lawes, English entrepreneur and agricultural scientist', 'Lindberg began his professional career at the age of 16, eventually moving to New York City in 1977.', \"``Boston (Ladies of Cambridge)'' by Vampire Weekend\", 'Gerald Francis Goyer (born October 20, 1936) was a professional ice hockey player who played 40 games in the National Hockey League.', 'He detected abnormalities of oxytocinergic function in schizoaffective mania, post-partum psychosis and how ECT modified oxytocin release.', 'With peak winds of 110 mph (175 km/h) and a minimum pressure of 972 mbar (hPa ; 28.71 inHg), Florence was the strongest storm of the 1994 Atlantic hurricane season.', 'He is currently a professor of piano at the University of Wisconsin -- Madison since August 2000.', 'In addition to known and tangible risks, unforeseeable black swan extinction events may occur, presenting an additional methodological problem.', 'Sir John Sherbrooke was able to hold her off for some five hours until Robson suffered a severe wound that almost killed him.', 'Matroid duals go back to the original paper by Hassler Whitney defining matroids.']\n",
            "['John Bennet Lawes, English entrepreneur and agricultural scientist', 'Lindberg began his professional career at the age of 16, eventually moving to New York City in 1977.', \"``Boston (Ladies of Cambridge)'' by Vampire Weekend\", 'Gerald Francis Goyer (born October 20, 1936) was a professional ice hockey player who played 40 games in the National Hockey League.', 'He detected abnormalities of oxytocinergic function in schizoaffective mania, post-partum psychosis and how ECT modified oxytocin release.', 'With peak winds of 110 mph (175 km/h) and a minimum pressure of 972 mbar (hPa ; 28.71 inHg), Florence was the strongest storm of the 1994 Atlantic hurricane season.', 'He is currently a professor of piano at the University of Wisconsin -- Madison since August 2000.', 'In addition to known and tangible risks, unforeseeable black swan extinction events may occur, presenting an additional methodological problem.', 'Sir John Sherbrooke was able to hold her off for some five hours until Robson suffered a severe wound that almost killed him.', 'Aslan Tlebzu (Аслъан ЛIыбзэу [- adyaːsɬaːn ɬʼəbzaw], Russian : Аслан Тлебзу), born 24 February 1981, Teuchezhsk, Adygea, USSR ; is a Russian Adyghe folk musician.']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "# remove non-english words\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)\n",
        "\n",
        "english_evidence = [sentence for sentence in full_evidence_text if is_pure_english(sentence)]\n",
        "count_word_eng = 0\n",
        "count_word = 0\n",
        "for evi in english_evidence:\n",
        "    count_word_eng+=1\n",
        "for evi in full_evidence_text:\n",
        "    count_word+=1\n",
        "print(\"word count: \",count_word)\n",
        "print(\"english word count: \",count_word_eng)\n",
        "print(\"word deleted: \",count_word-count_word_eng)\n",
        "print(\"word deleted percentage: \",(count_word-count_word_eng)/count_word)\n",
        "print(english_evidence[0:10])\n",
        "print(full_evidence_text[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[78], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m             keys\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keys\n\u001b[1;32m----> 8\u001b[0m english_evidence_id \u001b[38;5;241m=\u001b[39m \u001b[43mfind_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevi_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_evidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(english_evidence_id))\n",
            "Cell \u001b[1;32mIn[78], line 5\u001b[0m, in \u001b[0;36mfind_keys\u001b[1;34m(dictionary, target_list)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictionary\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m target_list:\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def find_keys(dictionary, target_list):\n",
        "    keys = []\n",
        "    for key, value in dictionary.items():\n",
        "        if value in target_list:\n",
        "            keys.append(key)\n",
        "    return keys\n",
        "\n",
        "english_evidence_id = find_keys(evi_data, english_evidence)\n",
        "print(len(english_evidence_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dev remove stop word会好一点， test不remove反而好\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "\n",
        "# claim_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "\n",
        "\n",
        "evidence_tfidf_vectorizer.fit(train_claim_text+full_evidence_text)\n",
        "train_claim_emb_list = claim_tfidf_vectorizer.fit_transform(train_claim_text)\n",
        "\n",
        "\n",
        "full_evi_emb_list = evidence_tfidf_vectorizer.transform(full_evidence_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1208827, 605251)\n"
          ]
        }
      ],
      "source": [
        "print(full_evi_emb_list.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "khk_kR5dnDh4"
      },
      "outputs": [],
      "source": [
        "\n",
        "evi_k=2\n",
        "claim_k=1\n",
        "\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    test_out_temp = json.load(input_file)\n",
        "\n",
        "for claim_id,claim_value in test_out_temp.items():\n",
        "    # Task1\n",
        "    # 把test claim转化成vector\n",
        "\n",
        "    test_claim_emb = evidence_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "    evi_sim_dict = {}\n",
        "    # 计算出test claim和所有evidence的相似度\n",
        "    sim = cosine_similarity(test_claim_emb, full_evi_emb_list)[0]\n",
        "\n",
        "    for i in range(len(sim)):\n",
        "        evi_sim_dict[full_evidence_id[i]] = sim[i]\n",
        "\n",
        "    # 对evidence根据和claim的相似度排序\n",
        "    s_sim = [(k, v) for k, v in sorted(evi_sim_dict.items(), key=lambda item: item[1],reverse=True)][:evi_k]\n",
        "    sel_sim = [k for k,v in s_sim]\n",
        "    # 把最相似的前k个evidence的id写入到test claim的evidence list\n",
        "    test_out_temp[claim_id][\"evidences\"] = sel_sim\n",
        "\n",
        "    # Task2\n",
        "    # 把test claim转化成vector 注意我这两问用了不同的vectorizer，因为有不同的预处理步骤，和应用数据目标\n",
        "    test_claim_emb = claim_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "\n",
        "    # 计算出test claim和所有train claim的相似度\n",
        "    claim_sim_dict = {}\n",
        "    claim_sim = cosine_similarity(test_claim_emb, train_claim_emb_list)[0]\n",
        "    for i in range(len(claim_sim)):\n",
        "        claim_sim_dict[train_claim_id[i]] = claim_sim[i]\n",
        "\n",
        "    # 取最相似的k个train claim\n",
        "    most_sim_claims = [(k, v) for k, v in sorted(claim_sim_dict.items(), key=lambda item: item[1],reverse=True)]\n",
        "    # 我这里用的是k=1只考虑最相似的那一个，label拿出来\n",
        "    most_sim_claim = max(most_sim_claims, key=operator.itemgetter(1))[0]\n",
        "\n",
        "    test_out_temp[claim_id][\"claim_label\"] = train_claim_data[most_sim_claim][\"claim_label\"]\n",
        "\n",
        "\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"data/dev_predict.json\", \"w\") as outfile:\n",
        "    json.dump(test_out_temp, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "d380c94d-3adf-4f7e-a751-cf737fe5a546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Evidence Retrieval F-score (F)    = 0.10510204081632651\\r\\nClaim Classification Accuracy (A) = 0.474025974025974\\r\\nHarmonic Mean of F and A          = 0.17205555936935077\\r\\n'\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
