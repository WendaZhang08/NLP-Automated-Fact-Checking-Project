{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "69cb3737-0ffc-4dce-f7cd-f133667067be"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRmNYMfb1g2"
      },
      "source": [
        "## 1.1 Reading and gathering data\n",
        "\n",
        "Using `json` package reading and gathering claims and evidences, then print an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1N-PVoLb1g2",
        "outputId": "44c25abc-6f6a-4a59-bcd5-ec0f17019121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 163\n",
            "Dev evi outside train evi 328\n",
            "Train claim count:  1228\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "\n",
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside += 1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n",
        "print(\"Train claim count: \",len(train_claim_id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCxJ35-b1g3"
      },
      "source": [
        "## 1.2 Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFcmSZmDlfE"
      },
      "source": [
        "### Implementing preprocessing fuctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYaLwC2MDlfF",
        "outputId": "7256f86e-88f2-4ae5-ae1b-65539597cb3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    return lemma if lemma != word else lemmatizer.lemmatize(word, 'n')\n",
        "\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)\n",
        "\n",
        "def remove_non_eng(dictionary):\n",
        "    eng_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if is_pure_english(value):\n",
        "            eng_data[key] = value\n",
        "    return eng_data\n",
        "\n",
        "def contains_climate_keywords(text, keywords):\n",
        "    text = text.lower()\n",
        "    for keyword in keywords:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", text):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def filter_climate_related(dictionary, keywords):\n",
        "    cs_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if contains_climate_keywords(value, keywords):\n",
        "            cs_data[key] = value\n",
        "    return cs_data\n",
        "\n",
        "def text_preprocessing(text, remove_stopwords=False):\n",
        "    words = [lemmatize(w) for w in text.lower().split()]\n",
        "    if remove_stopwords:\n",
        "        words = [w for w in words if w not in stopwords]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqq0xrC_DlfF"
      },
      "source": [
        "### Implementing **Claim data preprocessing** and **Evidence data preprocessing** functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nsxYPxqgDlfF"
      },
      "outputs": [],
      "source": [
        "# 权威网站 https://www.ucdavis.edu/climate/definitions\n",
        "climate_keywords = [\n",
        "    \"climate\", \"environment\", \"global warming\", \"greenhouse effect\", \"carbon\", \"co2\", \"carbon dioxide\",\n",
        "    \"methane\", \"renewable energy\", \"sustainability\", \"ecology\", \"biodiversity\", \"fossil fuels\",\n",
        "    \"emissions\", \"air quality\", \"ozone\", \"solar energy\", \"wind energy\", \"climate change\", \"climate crisis\",\n",
        "    \"climate adaptation\", \"climate mitigation\", \"ocean\", \"sea levels\", \"ice melting\", \"deforestation\",\n",
        "    \"reforestation\", \"pollution\"\n",
        "]\n",
        "\n",
        "def filter_evidence_by_train(train_claim_data, evidence_data):\n",
        "\n",
        "    # Collect all evidence ids in the training set\n",
        "    train_evidence_ids = set()\n",
        "\n",
        "    for claim in train_claim_data.values():\n",
        "        train_evidence_ids.update(claim['evidences'])\n",
        "\n",
        "    # filter evidence data by the evidence ids in the training set\n",
        "    filtered_evidence_data = {key: value for key, value in evidence_data.items() if key in train_evidence_ids}\n",
        "\n",
        "    return filtered_evidence_data\n",
        "\n",
        "def preprocess_claim_data(claim_data, existed_evidences_id=None):\n",
        "    claim_data = remove_non_eng(claim_data)\n",
        "    claim_data_text = []\n",
        "    claim_data_id = []\n",
        "    claim_data_label = []\n",
        "    claim_evidences = []\n",
        "\n",
        "    for key in claim_data.keys():\n",
        "        claim_data[key][\"claim_text\"] = text_preprocessing(claim_data[key][\"claim_text\"])\n",
        "        claim_data_text.append(claim_data[key][\"claim_text\"])\n",
        "        claim_data_id.append(key)\n",
        "\n",
        "        if \"claim_label\" in claim_data[key]:\n",
        "            claim_data_label.append(claim_data[key][\"claim_label\"])\n",
        "        else:\n",
        "            claim_data_label.append(None)\n",
        "\n",
        "        if existed_evidences_id and \"evidences\" in claim_data[key]:\n",
        "            claim_evidences.append([existed_evidences_id[i] for i in claim_data[key][\"evidences\"]])\n",
        "        else:\n",
        "            claim_evidences.append([])\n",
        "\n",
        "    return claim_data_text, claim_data_id, claim_data_label, claim_evidences\n",
        "\n",
        "def preprocess_evi_data(evi_data, climate_keywords, train_claim_data):\n",
        "    evi_data = remove_non_eng(evi_data)\n",
        "    cs_evi_data = filter_climate_related(evi_data, climate_keywords)\n",
        "\n",
        "    # filter evidence data by the evidence ids in the training set\n",
        "    train_evi_data = filter_evidence_by_train(train_claim_data, cs_evi_data)\n",
        "\n",
        "    for key in train_evi_data.keys():\n",
        "        train_evi_data[key] = text_preprocessing(train_evi_data[key], remove_stopwords=True)\n",
        "\n",
        "    cleaned_evidence_text = list(train_evi_data.values())\n",
        "    cleaned_evidence_id = list(train_evi_data.keys())\n",
        "\n",
        "    return cleaned_evidence_text, cleaned_evidence_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8bIR_XDlfF"
      },
      "source": [
        "### Start dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0_ht7aH1DlfF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the claim data, split the data into text, id, label and evidences\n",
        "train_claim_text, train_claim_id, train_claim_label, train_claim_evidences = preprocess_claim_data(train_claim_data)\n",
        "\n",
        "dev_claim_text, dev_claim_id, dev_claim_label, dev_claim_evidences = preprocess_claim_data(dev_claim_data)\n",
        "\n",
        "test_claim_text, test_claim_id, _, _ = preprocess_claim_data(test_claim_data)\n",
        "\n",
        "# Preprocessing the evidence data, split the data into text and id\n",
        "cleaned_evidence_text, cleaned_evidence_id = preprocess_evi_data(evi_data, climate_keywords, train_claim_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train claim count:  1228\n"
          ]
        }
      ],
      "source": [
        "print(\"Train claim count: \",len(train_claim_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3LL9-aEb1g5"
      },
      "source": [
        "## 1.3 Development Set Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4cbdtTb1g5"
      },
      "source": [
        "In this section, we perform the main tasks of the project on the development set:\n",
        "\n",
        "1. **Evidence Retrieval**: For each claim, find the most relevant evidence from the corpus.\n",
        "2. **Claim Classification**: Predict the label for each claim based on the retrieved evidence and the claim's similarity to the training claims.\n",
        "\n",
        "The code uses TF-IDF vectorization and cosine similarity to measure the relevance between claims and evidence, and between development and training claims. The most similar evidence and training claims are used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Creating two vectorizer\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
        "\n",
        "# fit the vectorizer on the evidence data\n",
        "evidence_tfidf_vectorizer.fit(cleaned_evidence_text)\n",
        "\n",
        "# Transform cleaned_evidence_text\n",
        "transformed_evidence = evidence_tfidf_vectorizer.transform(cleaned_evidence_text)\n",
        "\n",
        "# Transform claim data\n",
        "train_claim_tfidf = evidence_tfidf_vectorizer.transform(train_claim_text)\n",
        "dev_claim_tfidf = evidence_tfidf_vectorizer.transform(dev_claim_text)\n",
        "test_claim_tfidf = evidence_tfidf_vectorizer.transform(test_claim_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "58n5gcslrAmV"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between train claims and evidence\n",
        "train_similarity = cosine_similarity(train_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between dev claims and evidence\n",
        "dev_similarity = cosine_similarity(dev_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between test claims and evidence\n",
        "test_similarity = cosine_similarity(test_claim_tfidf, transformed_evidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fzgteUN7rAmX"
      },
      "outputs": [],
      "source": [
        "def spliting_dataset(similarity, claim_texts, claim_evidences, evidence_texts, top_k=5, neg_ratio=1):\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Based on the similarity matrix, find the top k most similar evidence for each claim\n",
        "    for i in range(similarity.shape[0]):\n",
        "\n",
        "        claim_text = claim_texts[i]\n",
        "\n",
        "        # Find the top k most similar evidence\n",
        "        top_evidences = np.argsort(-similarity[i])[:top_k]\n",
        "\n",
        "        # Add the top k most similar evidence to the dataset, label as 1\n",
        "        for evidence_index in top_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(1)\n",
        "\n",
        "        # If the claim has evidences, add the evidence to the dataset, label as 1\n",
        "        if claim_evidences is not None:\n",
        "            for evidence_index in claim_evidences[i]:\n",
        "                evidence_text = evidence_texts[evidence_index]\n",
        "                dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "                labels.append(1)\n",
        "\n",
        "        # Randomly sample negative samples, label as 0\n",
        "        neg_samples_num = int(neg_ratio * len(top_evidences))\n",
        "\n",
        "        # Randomly sample negative samples that are not in the top k most similar evidence\n",
        "        neg_evidences = np.random.choice(\n",
        "            [j for j in range(similarity.shape[1]) if j not in top_evidences],\n",
        "            neg_samples_num\n",
        "        )\n",
        "\n",
        "        # Add the negative samples to the dataset\n",
        "        for evidence_index in neg_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(0)\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1yZ4FGlrrAmX"
      },
      "outputs": [],
      "source": [
        "train_dataset, train_dataset_labels = spliting_dataset(\n",
        "    train_similarity, train_claim_text, train_claim_evidences, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")\n",
        "dev_dataset, dev_dataset_labels = spliting_dataset(\n",
        "    dev_similarity, dev_claim_text, None, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")\n",
        "test_dataset, test_dataset_labels = spliting_dataset(\n",
        "    test_similarity, test_claim_text, None, cleaned_evidence_text, top_k=5, neg_ratio=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZxkXsHUgb1g7"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset labels to numpy array\n",
        "train_label_array = np.array(train_dataset_labels)\n",
        "dev_label_array = np.array(dev_dataset_labels)\n",
        "test_label_array = np.array(test_dataset_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "a9pkJPuab1g8"
      },
      "outputs": [],
      "source": [
        "# need to install\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ICh3nDmFb1g8"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  # 0 is padding token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "W2Sj6qFub1g8"
      },
      "outputs": [],
      "source": [
        "# Convert the text data to sequence\n",
        "train_sequence = tokenizer.texts_to_sequences(train_dataset)\n",
        "dev_sequence = tokenizer.texts_to_sequences(dev_dataset)\n",
        "test_sequence = tokenizer.texts_to_sequences(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KCi9XwMPb1g8"
      },
      "outputs": [],
      "source": [
        "longest_train_sequence = 0\n",
        "for i in train_sequence:\n",
        "    longest_train_sequence = max(longest_train_sequence, len(i))\n",
        "\n",
        "longest_dev_sequence = 0\n",
        "for i in dev_sequence:\n",
        "    longest_dev_sequence = max(longest_dev_sequence, len(i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "WQyglIFSb1hA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_length = max(longest_train_sequence, longest_dev_sequence) + 5\n",
        "\n",
        "padded_train_sequence = pad_sequences(train_sequence, maxlen=padding_length, padding='post')\n",
        "padded_dev_sequence = pad_sequences(dev_sequence, maxlen=padding_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG_0JSzdb1hA",
        "outputId": "76b02cfc-1689-480e-d585-60c369ceda48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"retrieval_cls_lstm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 244, 200)          1160800   \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 244, 200)          0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 244, 800)         1923200   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d_5 (Glo  (None, 800)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 400)               320400    \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 400)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 401       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,404,801\n",
            "Trainable params: 3,404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# from workshop\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "embedding_dim = 200\n",
        "hidden_dim = 400\n",
        "\n",
        "#model definition\n",
        "# feedforward network (MLP)\n",
        "model = Sequential(name=\"retrieval_cls_lstm\")\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=padding_length, embeddings_regularizer=l2(0.02)))\n",
        "\n",
        "model.add(layers.Dropout(0.6))\n",
        "# model.add(LSTM(hidden_dim, return_sequences=True, dropout=0.1))\n",
        "# model.add(LSTM(hidden_dim, dropout=0.1))\n",
        "\n",
        "model.add(layers.Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.6, kernel_regularizer=l2(0.02), recurrent_regularizer=l2(0.02))))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(layers.Dense(hidden_dim, activation='tanh', kernel_regularizer=l2(0.02), bias_regularizer=l2(0.02)))\n",
        "model.add(layers.Dropout(0.6))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.Recall()])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "decay_steps = 3000\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    learning_rate, decay_steps\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2RL1LOb1hA",
        "outputId": "ff3742dd-d3bd-40e0-888a-e6d846e76f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "192/192 [==============================] - 13s 55ms/step - loss: 34.7774 - val_loss: 17.2041\n",
            "Epoch 2/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 8.8305 - val_loss: 3.4120\n",
            "Epoch 3/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 1.7339 - val_loss: 0.9068\n",
            "Epoch 4/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.7600 - val_loss: 0.7031\n",
            "Epoch 5/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6971 - val_loss: 0.6941\n",
            "Epoch 6/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6936 - val_loss: 0.6933\n",
            "Epoch 7/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6933 - val_loss: 0.6932\n",
            "Epoch 8/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6933 - val_loss: 0.6932\n",
            "Epoch 9/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 10/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 11/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 12/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 13/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 14/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 15/15\n",
            "192/192 [==============================] - 10s 51ms/step - loss: 0.6932 - val_loss: 0.6932\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x22c174c0160>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(padded_train_sequence,train_label_array,epochs=15,validation_data=(padded_dev_sequence, dev_label_array),verbose=True,batch_size=64,callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cUG6D3SPrAmZ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "# model.save('retrieval_cls_lstm')\n",
        "\n",
        "# Load the model\n",
        "# model = tf.keras.models.load_model('retrieval_cls_lstm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "20d96abf-cae9-4441-f21e-238786b63b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F): 0.04555246340960627\n",
            "Claim Classification Accuracy (A): 0.38961038961038963\n",
            "Harmonic Mean of F and A: 0.08156814348266166\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "output_str = output.decode('utf-8')\n",
        "\n",
        "# Split the output into lines\n",
        "output_lines = output_str.strip().split('\\n')\n",
        "\n",
        "# Format the output\n",
        "formatted_lines = []\n",
        "for line in output_lines:\n",
        "    metric, value = line.split('=')\n",
        "    metric = metric.strip()\n",
        "    value = value.strip()\n",
        "    formatted_line = f\"{metric}: {value}\"\n",
        "    formatted_lines.append(formatted_line)\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "formatted_output = '\\n'.join(formatted_lines)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
