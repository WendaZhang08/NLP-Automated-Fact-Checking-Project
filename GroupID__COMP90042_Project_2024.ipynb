{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "56611668-da3f-4157-9707-111dd751cc29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRmNYMfb1g2"
      },
      "source": [
        "## 1.1 Reading and gathering data\n",
        "\n",
        "Using `json` package reading and gathering claims and evidences, then print an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1N-PVoLb1g2",
        "outputId": "15670b2e-7fef-46f2-ae24-dea4041f043a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 0\n",
            "Dev evi outside train evi 154\n",
            "Train claim count:  1228\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "\n",
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside += 1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n",
        "print(\"Train claim count: \",len(train_claim_id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCxJ35-b1g3"
      },
      "source": [
        "## 1.2 Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFcmSZmDlfE"
      },
      "source": [
        "### Implementing preprocessing fuctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYaLwC2MDlfF",
        "outputId": "10c41a64-59c6-428f-88c3-7a11e794c2b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    return lemma if lemma != word else lemmatizer.lemmatize(word, 'n')\n",
        "\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)\n",
        "\n",
        "def remove_non_eng(dictionary):\n",
        "    eng_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if is_pure_english(value):\n",
        "            eng_data[key] = value\n",
        "    return eng_data\n",
        "\n",
        "def contains_climate_keywords(text, keywords):\n",
        "    text = text.lower()\n",
        "    for keyword in keywords:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", text):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def filter_climate_related(dictionary, keywords):\n",
        "    cs_data = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if contains_climate_keywords(value, keywords):\n",
        "            cs_data[key] = value\n",
        "    return cs_data\n",
        "\n",
        "def text_preprocessing(text, remove_stopwords=False):\n",
        "    words = [lemmatize(w) for w in text.lower().split()]\n",
        "    if remove_stopwords:\n",
        "        words = [w for w in words if w not in stopwords]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqq0xrC_DlfF"
      },
      "source": [
        "### Implementing **Claim data preprocessing** and **Evidence data preprocessing** functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nsxYPxqgDlfF"
      },
      "outputs": [],
      "source": [
        "# 权威网站 https://www.ucdavis.edu/climate/definitions\n",
        "climate_keywords = [\n",
        "    \"climate\", \"environment\", \"global warming\", \"greenhouse effect\", \"carbon\", \"co2\", \"carbon dioxide\",\n",
        "    \"methane\", \"renewable energy\", \"sustainability\", \"ecology\", \"biodiversity\", \"fossil fuels\",\n",
        "    \"emissions\", \"air quality\", \"ozone\", \"solar energy\", \"wind energy\", \"climate change\", \"climate crisis\",\n",
        "    \"climate adaptation\", \"climate mitigation\", \"ocean\", \"sea levels\", \"ice melting\", \"deforestation\",\n",
        "    \"reforestation\", \"pollution\"\n",
        "]\n",
        "\n",
        "# def filter_evidence_by_train(train_claim_data, evidence_data):\n",
        "\n",
        "#     # Collect all evidence ids in the training set\n",
        "#     train_evidence_ids = set()\n",
        "\n",
        "#     for claim in train_claim_data.values():\n",
        "#         train_evidence_ids.update(claim['evidences'])\n",
        "\n",
        "#     # filter evidence data by the evidence ids in the training set\n",
        "#     filtered_evidence_data = {key: value for key, value in evidence_data.items() if key in train_evidence_ids}\n",
        "\n",
        "#     return filtered_evidence_data\n",
        "\n",
        "def preprocess_claim_data(claim_data, existed_evidences_id=None):\n",
        "    claim_data = remove_non_eng(claim_data)\n",
        "    claim_data_text = []\n",
        "    claim_data_id = []\n",
        "    claim_data_label = []\n",
        "    claim_evidences = []\n",
        "\n",
        "    for key in claim_data.keys():\n",
        "        claim_data[key][\"claim_text\"] = text_preprocessing(claim_data[key][\"claim_text\"])\n",
        "        claim_data_text.append(claim_data[key][\"claim_text\"])\n",
        "        claim_data_id.append(key)\n",
        "\n",
        "        if \"claim_label\" in claim_data[key]:\n",
        "            claim_data_label.append(claim_data[key][\"claim_label\"])\n",
        "        else:\n",
        "            claim_data_label.append(None)\n",
        "\n",
        "        if existed_evidences_id and \"evidences\" in claim_data[key]:\n",
        "            valid_evidences = [existed_evidences_id[i] for i in claim_data[key][\"evidences\"] if i in existed_evidences_id]\n",
        "            claim_evidences.append(valid_evidences)\n",
        "        else:\n",
        "            claim_evidences.append([])\n",
        "\n",
        "    return claim_data_text, claim_data_id, claim_data_label, claim_evidences\n",
        "\n",
        "# def preprocess_evi_data(evi_data, climate_keywords, train_claim_data):\n",
        "#     evi_data = remove_non_eng(evi_data)\n",
        "#     # cs_evi_data = filter_climate_related(evi_data, climate_keywords)\n",
        "\n",
        "#     # filter evidence data by the evidence ids in the training set\n",
        "#     # train_evi_data = filter_evidence_by_train(train_claim_data, cs_evi_data)\n",
        "\n",
        "#     for key in evi_data.keys():\n",
        "#         evi_data[key] = text_preprocessing(evi_data[key], remove_stopwords=True)\n",
        "\n",
        "#     cleaned_evidence_text = list(evi_data.values())\n",
        "#     cleaned_evidence_id = list(evi_data.keys())\n",
        "\n",
        "#     return cleaned_evidence_text, cleaned_evidence_id\n",
        "\n",
        "def preprocess_evi_data(evi_data, climate_keywords):\n",
        "    evi_data = remove_non_eng(evi_data)\n",
        "    cs_evi_data = filter_climate_related(evi_data, climate_keywords)\n",
        "\n",
        "    # filter evidence data by the evidence ids in the training set\n",
        "    # train_evi_data = filter_evidence_by_train(train_claim_data, cs_evi_data)\n",
        "\n",
        "    for key in cs_evi_data.keys():\n",
        "        cs_evi_data[key] = text_preprocessing(cs_evi_data[key], remove_stopwords=True)\n",
        "\n",
        "    cleaned_evidence_text = list(cs_evi_data.values())\n",
        "    cleaned_evidence_id = list(cs_evi_data.keys())\n",
        "\n",
        "    return cleaned_evidence_text, cleaned_evidence_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8bIR_XDlfF"
      },
      "source": [
        "### Start dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0_ht7aH1DlfF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the evidence data, split the data into text and id\n",
        "# cleaned_evidence_text, cleaned_evidence_id = preprocess_evi_data(evi_data, climate_keywords, train_claim_data)\n",
        "\n",
        "cleaned_evidence_text, cleaned_evidence_id = preprocess_evi_data(evi_data, climate_keywords)\n",
        "\n",
        "# Create a dictionary to map evidence ID to index\n",
        "evidences_id_dict = {evidence_id: index for index, evidence_id in enumerate(cleaned_evidence_id)}\n",
        "\n",
        "# Preprocessing the claim data, split the data into text, id, label and evidences\n",
        "train_claim_text, train_claim_id, train_claim_label, train_claim_evidences = preprocess_claim_data(train_claim_data, evidences_id_dict)\n",
        "\n",
        "dev_claim_text, dev_claim_id, dev_claim_label, dev_claim_evidences = preprocess_claim_data(dev_claim_data, evidences_id_dict)\n",
        "\n",
        "test_claim_text, test_claim_id, _, _ = preprocess_claim_data(test_claim_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  [[7086, 19094, 213], [5473, 18047], [8496], [18805, 12603, 8653, 5674, 16135], [], [3611, 7893, 9796], [], [481], [], [7026], [3469], [9634], [6203, 17249, 15630], [8271, 16319, 12741, 16160, 14064], [7959], [], [2803, 8271, 16160], [5900, 18382, 7339, 1465, 13080], [], [19217], [2928], [14103], [4982], [], [18927], [], [16204], [], [12324], [], [5360], [], [15220, 11696], [1127], [], [6073, 1905, 14345, 12566, 13909], [], [12721], [], [1995, 12306, 15589], [18052, 10626, 17108], [5900, 1079, 7706], [16422], [10899, 11259], [17772, 14807, 8636, 11955], [18695, 7742, 18854], [], [10254, 4637], [], [10097, 6312], [3502, 12407], [], [9727], [13534, 11512, 3499, 7706], [], [6183, 2258, 9939], [7659], [], [6682], [17865], [13413, 18047, 6129, 11650], [1119], [], [1480, 5637], [5900, 2959, 13093, 16657, 15223], [4266], [18805, 4123], [], [18280, 10549, 11802], [], [11041], [11955], [], [4013, 14899], [], [], [3842, 12507], [9868, 14364], [14345], [720], [18047], [], [], [18506], [293], [13093, 11955, 293], [10991, 7110], [18917, 15993, 18854], [13405, 11696], [], [], [], [3575], [1810], [14757, 17336], [2791], [546, 14927, 17621, 2584], [11955, 7403], [4569, 420], [], [], [5900, 12890, 13093, 1079], [], [18413, 1810], [3116, 5508, 12210], [15668], [6357, 15215, 644, 17362, 1973], [], [11955], [2219], [], [4860], [17118], [], [15350], [2302, 12977], [], [], [18977, 3357, 8354, 17106, 968], [], [], [18583], [492, 16747, 452, 5935], [481], [11955, 293], [8281], [11242], [], [], [12476, 14965, 3505, 16789, 8210], [], [], [11660, 13093, 17043, 11517], [17867, 8182, 6936], [5193, 14594, 9496, 7238], [8912, 1154, 18280], [509, 5574, 6447], [9846, 11387, 5117], [14936, 19262], [], [10114, 16271], [4346], [], [6392], [], [], [], [], [], [17043, 11517, 599, 5756], [4123, 465], [9610, 10210, 8702], [5009, 18935], [1499], [], [16298], [8550, 5900, 13093], [10021], [], [], [], [8186, 10017, 15301, 3994, 13358], [], [], [17883, 16366], [18814, 9435, 7438, 4421], [], [7838, 16565, 5473], [], [10491], [13660, 1973], [], [19169, 10591, 14170, 5127], [], [5218, 18250, 533, 1879, 5433], [], [7328, 8550, 7897], [], [16315, 11545, 10039, 9092, 13997], [12759], [], [14184], [], [9227, 19251, 2822], [7485], [3948], [17143, 8521, 7805, 14890, 17966], [1905, 13137, 14738, 18047], [], [], [4625], [3605, 13670], [7659], [8137, 13816], [4637, 16258], [12464, 4198], [11288, 1609], [], [], [16155, 2790], [], [12459, 10867], [], [], [6787], [19123, 4120], [], [], [], [945, 16756], [7650, 6682, 4905], [18337, 13593], [18005], [14628, 6174], [7479, 17474], [13413, 18047, 4102], [13863, 1443, 9298, 8949, 4890], [15598], [5656, 12069, 1514], [19145], [3468, 3768, 10370, 12435], [15990, 5473], [], [1106, 3502, 6681, 11955], [19145, 16291, 6975], [], [11512], [], [11369], [], [13796, 15967, 1383, 5468, 9037], [13142, 18112, 11955, 7438], [13464], [], [], [], [], [9610, 10011, 1079, 2308, 9233], [14651, 5842], [14679], [13093, 1079, 16083, 2270, 14905], [], [], [2204, 19296, 4938], [], [6921, 4097, 18363], [17541], [7641, 8520], [8109], [], [], [11807, 15937, 4379, 1364, 17196], [2534, 7006], [6331, 16581], [12281], [18885, 9461, 19016, 10750, 795], [], [704, 2365, 2037], [10235, 17118, 7317], [15785, 151], [], [], [3638], [13413, 18047], [7842, 1042], [11939], [18695, 19217, 7742, 18854], [], [], [17883, 15688, 11955, 2536, 7524], [], [], [], [], [], [1106, 4311, 16538], [873], [], [10550, 17470, 1802], [], [3489], [], [2204, 1735, 15650], [17249, 2934, 9259, 1319, 18951], [], [9215], [12177], [], [12071], [2385], [12602], [3372, 8070], [], [], [8820, 9414, 5185], [18545], [], [], [12650, 17218, 19094, 1348, 3270], [8091], [15506], [], [], [3043, 369, 12169, 70, 2110], [5485], [16040, 3202, 15041], [13137, 8439, 18047], [5574], [18339, 7909, 1606], [14575], [15598, 12287, 19119, 3845], [1271, 1319], [16252], [3503, 18460, 5900, 17835, 12475], [965], [7450, 1030], [], [6141, 2433], [686], [], [9698, 16899, 16752, 12213, 14070], [16878, 1905, 13137], [], [], [13143], [], [], [], [15329], [], [6136], [5635, 4318], [], [7316], [2464], [6203, 14345, 3562], [9883, 15556, 11955], [11857, 3663], [], [18805, 13750, 15432, 6203, 7317], [6615], [], [2559], [15319, 18521, 7817, 12804], [], [3468, 8498, 10370, 12435], [13414, 6364], [7529, 19119, 3845], [4970], [19123, 17313, 18804, 6581], [], [13005, 13093], [], [11955, 18977, 19217, 7742, 18854], [6843], [7086], [3471, 11948, 10794, 10523], [], [13711, 5900, 16657, 12237, 11259], [9373], [3165, 13289, 3699, 9155], [], [], [18145], [7450, 2240, 4346, 5473], [], [10219, 2328], [708, 809], [1460], [], [5048], [2715, 18226], [14064], [4657, 9663], [], [], [2505, 16121], [], [10505, 8070, 12770], [], [3684], [12563], [11955, 13513], [], [], [], [4258, 2715, 15598], [], [], [9779, 3424, 1122], [14339], [17408], [], [16197, 4903, 7095, 4331], [], [18573, 12716, 1478, 906], [3851], [], [], [], [], [3842, 1905, 13137], [6558, 11517, 12237, 8914, 11259], [], [17121, 6136], [7442, 11002, 3998, 5473], [], [11955, 18413, 14103], [], [10609, 6585], [], [4861, 12093, 15218, 4733], [], [], [8945, 5805, 15223, 1966], [13093, 15688, 15218, 17249, 11242], [5900, 7339, 15993, 14905], [15668], [3962, 11242, 16765, 7285, 679], [], [], [5018, 12808, 13159, 981], [12555], [11517, 599, 12303], [13093, 15688], [16938], [293], [13625, 8034], [1119], [17302], [12507, 18927], [3475, 11881, 11068, 8874, 14278], [15218], [19217], [], [18805, 7438, 1681, 3998, 18854], [451], [1421, 6160], [], [18499, 12706, 1815, 7247], [], [2846, 15506, 7659, 481, 19145], [10720, 15220], [12476, 14556, 3505, 8210, 13420], [5133], [11198, 8503, 18814, 1629], [15197], [11170, 8124, 6055, 7438, 16129], [13955, 18744, 17599, 12402], [13718, 2640], [13263, 7764, 18768], [7067], [1872], [14498, 7078], [10609, 17282], [], [], [18038], [481, 16291], [], [15319, 14757, 860, 2525, 4803], [14908], [], [10296, 11275], [9912, 95, 994], [2601], [18626, 8503], [16530, 11102], [17994, 16488, 8503, 5641, 7684], [9093], [6047], [17118, 5130], [16173], [1079], [7611], [2072, 16530, 6887], [11015, 14351, 17838, 5737], [], [], [], [18515, 7584, 6033, 16637, 13025], [], [8121], [], [13005, 2989, 14850, 16708], [6653, 12322], [19022, 18781], [], [], [18721, 18768], [], [9814], [], [7322, 7507, 3783], [2976], [], [19230], [], [], [], [9988], [1802], [3492, 4569, 1353, 1822], [6884, 6336, 7810], [12517, 4522, 116, 8915], [], [], [], [15767, 18270, 1776, 11628, 4849], [6985], [], [], [11966, 17074], [13810, 18959, 11558, 7317, 17470], [8668], [8847, 2237, 10275, 14296], [7891], [], [], [10402], [17700], [9956], [13731], [2601], [], [16446, 18683, 2031], [5900], [80], [], [14757, 1995, 1079, 3475, 16871], [], [], [], [], [], [3489, 13362], [], [], [4147], [], [13505, 5109], [], [7651, 18977, 12198], [5900, 12890, 6787], [205, 4722], [], [13660, 8087], [13650, 8867], [], [], [6203, 18413, 1810], [], [14651], [2505, 9222], [5900], [10468], [1594, 11955, 8070, 12770], [2878, 6580], [19046, 18604], [], [14651, 16527, 14116], [], [1117], [], [14144], [14651, 14382, 17500], [17517], [12102, 13243, 2380], [14626], [10381, 2099, 5012], [15096, 17819, 7627], [4743], [1904], [18280, 14208, 11431, 18767], [], [18944, 15891, 6934, 1840], [6386, 14905], [7210, 18917], [4446, 4474], [], [], [11955, 17744], [7714, 5527, 17962, 5805, 8653], [11939, 16245], [15507, 151], [], [13795, 2443, 6227], [18337], [4851], [], [], [], [17412, 6433], [13137], [8234], [19145], [17883], [16753, 7674], [17092, 14651, 15598, 9158], [13093], [4743], [16263, 12452], [17478, 11199], [6257], [], [16985, 10572, 13285, 13900, 6385], [17335, 7794, 11115], [2715], [19145, 16291, 6054], [5797, 14651], [19296, 14811], [], [], [17636, 13609, 7469, 11242, 599], [], [9464], [3562, 712], [], [16804], [], [14419, 6409, 4515, 18854], [2518, 10073], [6902, 18179, 392], [7479, 1354], [8314], [5191], [12690], [11880], [], [9158], [91, 2464, 16884, 15570], [10614], [15602, 614, 12718, 13050], [], [9625, 7659, 12619], [11955, 8209, 17196, 7742], [], [8433, 16878, 11300], [7831, 17196], [], [1748], [], [9742, 14936], [16291], [4339], [18513], [], [], [14651, 11190], [], [4219], [14231, 3533], [], [], [2514, 3729, 18412], [12690, 10439], [16263, 12864], [3962, 17974, 11550], [], [3275, 17210], [], [], [14936], [18927], [7461], [7706, 11550], [], [2690], [1594, 12555, 1354, 11955], [17609, 15718, 693, 3053], [], [1692, 12237, 3409], [], [7262], [16918, 6729, 8866, 17612, 4445], [], [], [5205], [], [5130], [8290, 912], [1146], [1387, 5788, 12771, 2270, 1453], [14917, 1928], [4119], [10869, 11172], [7997, 1143], [963], [332, 10033, 4884], [], [10056], [8070], [3475, 11881, 5749], [3239], [], [5717, 17665, 10505], [6203], [4803], [], [5601, 13865, 94], [11426, 16867], [], [11955], [17517, 11955], [11955], [], [16008, 1802, 9093], [481, 2957, 11386, 8345, 16797], [9190], [2880], [11955], [16216], [], [14403], [], [4684, 17082, 12550, 16598, 5469], [18805], [], [9625], [13016, 17883, 14345, 11955], [], [18649, 1815, 7053, 3961], [15792, 5900, 12093], [12914], [7844, 7949, 12903, 9028, 13581], [1354, 1446, 11955, 7073, 2433], [11955, 18977, 10390, 3771, 2106], [13005, 11955, 14103, 3771], [12951, 11955, 7897], [12721], [], [3222], [], [5133, 1165, 1890, 6392], [14498], [686], [13137, 2715, 2601], [3294, 12706, 7469, 10653], [14052, 7861, 10716], [12706, 7053], [], [18112], [], [8867, 5909], [], [], [], [], [14996, 15393, 12968, 14039], [], [10777, 13051], [17541, 14936, 19145], [], [], [5009, 12119, 324, 18935], [], [], [14757, 17336, 9095], [4065, 6087], [16339, 14278, 2924, 4823], [15506, 8503, 7659, 481], [2270, 15372, 6054, 4346], [], [], [], [1154], [11259], [], [18805, 11955, 11242], [11955, 6787, 5473], [14010, 16654], [5978, 18327], [], [], [17249, 8076, 5718, 9497, 10505], [7412, 2343, 7336], [15389, 9864, 6203], [], [], [16441], [3076], [11465, 17788, 668], [], [14470, 1453, 7218, 4563], [], [8047, 1165, 1890, 10502], [13593], [7788], [], [9083, 11064], [7343, 7659, 481, 16800], [2280], [4266], [], [], [6545, 848], [], [], [13711, 16132, 5900], [1280], [], [4512], [14434, 7461], [1858, 12557, 13289], [8234], [], [], [3842, 13137, 18927, 7742], [4415], [5082, 15020], [], [], [8284, 17840], [5140], [11512, 10137, 7706], [1759], [], [18375], [11110, 13790], [], [], [], [16228, 938, 508], [1107], [587], [], [], [], [], [8867, 7716], [], [2623, 11955, 15218, 3771], [4013], [1543, 14418, 16665], [], [], [], [19073], [18680, 16252, 15369, 6864], [12324, 425, 4803], [], [7661, 2220, 12144], [], [], [], [], [6118, 19145, 5767, 8915], [3738], [4869], [481], [1042, 11163], [], [], [], [], [446], [1904], [], [], [7842], [14396, 9190], [], [2260], [7113, 11000], [10502], [], [], [8484, 16598, 981, 5469, 1531], [18047, 9373], [16765, 7285, 960], [14103, 18854], [], [17092, 13045, 15306], [], [16488, 18489], [11668, 15474], [363, 9924, 15370], [11708], [10449, 7655, 8891], [13787, 7981, 8363, 1326], [], [], [14305, 14800], [2400], [7233], [], [8284, 17840, 4979], [6530, 5350, 12465], [], [], [2514], [], [], [12968, 12281], [5133, 7450, 10502], [11955, 6787, 18047], [1890], [], [18723, 13348, 91, 15990], [4625, 7716], [7135], [5900, 6197, 5238], [], [15218, 13231], [481], [], [], [1165], [], [], [16062], [], [4756, 6069, 8338, 8138], [2959, 9603], [12759, 7742], [], [5218, 18250, 16135], [492, 19118, 18304], [], [13639, 13137, 6975], [7170], [15364], [2976], [8550, 15598, 10089], [17046], [8661], [5274, 11387], [1773, 1543, 9914, 1863], [], [], [], [19153], [15816, 2854, 1761], [], [18677, 11653, 2052, 7317, 481], [], [11767, 11955, 3215, 6787], [5900, 9227, 1586, 14351, 19110], [15356, 18977], [7322, 10550], [17408], [6829, 18413, 1810], [], [19053, 7715], [575, 9321], [5133, 1890], [13093, 4258], [7912], [11389], [10633], [2248, 4653, 15311, 4183], [], [], [], [9115], [4383], [17404], [15994, 1239, 4553], [5527, 12407], [14827, 15218, 18295], [], [], [16753, 7450, 14103, 10175], [16737, 4451, 13110], [2955, 17290], [9742, 14936], [], [91, 16459, 14811, 12520, 18844], [], [5900, 12890, 4498, 4656, 11955], [13054], [15329, 13142, 11955], [18337], [3340], [5347], [16594, 14824], [], [], [9423, 11807, 18534, 4691, 16867], [1165], [8070, 1879, 16135], [2031], [16784, 16073], [928], [3499, 15428, 19145], [5900, 6787], [10449], [12891, 5158, 4346], [8225, 17894], [], [3605], [], [13065], [6791], [8781], [19050], [], [1890, 10502], [17497], [6109], [6816, 15319, 3294, 17636], [], [], [2861], [10556, 11809, 12759], [6818], [], [], [], [2222], [13130, 7909], [], [11491, 13660], [14722, 6109, 18364, 13177], [16591, 13816, 2270], [16228, 508, 13137], [], [], [12324, 15688], [13142, 8820], [6682, 14628], [18526], [11863, 1983], [2791], [730, 637, 12093], [17118], [1444, 7429], [1574, 10626, 10907], [], [], [11955, 18977], [11796, 8557], [15080], [511], [5059], [4165], [1994], [3489, 13362], [686], [6936], [17978, 17356, 15329, 2107, 10285], [], [17701], [], [], [15990, 2031], [], [15319, 18521, 12804], [], [5076], [19149, 10550], [], [], [3948], [4268], [13093, 13593, 14905, 17582], [15242], [], [], [17936, 14176, 15372], [5476], [], [4876, 472], [11120], [15398, 7987, 7593, 19186, 18186], [], [5197, 2270], [], [8653, 15045, 18424], [4966, 4266], [15139, 18354, 11855], [], [], [18966, 2972], [], [704, 2037], [], [2204, 7680], [1100, 4221], [5900, 13093], [4060, 19217], [], [], [], [], [8795, 8499], [10384], [2913], [11886], [16212, 18875, 18165], [12202, 5425], [11955], [4319], [16097, 8950], [16388], [], [], [], [10694, 499, 11955], [5564, 5942, 13963, 9142], [1917, 19153, 18238], [], [15031, 17470, 1802, 12721], [], [], [10631, 15329, 14911, 12968], [19247, 3040, 1229], [1229], [], [], [], [16099, 1606], [], [15901], [], [12827, 12603, 3692, 15284], [8239], [7650, 10906], [], [], [3357], [], [18337], [756], [3644, 3983, 16803], [], [10368, 18705], [], [], [1242], [10022], [17936], [11955, 10549], [14891, 1361], [2270, 12706], [7578], [9603, 4346, 19217, 1810, 4223], [18047], [17986], [], [16753, 14896, 5473], [], [], [], [9914], [7442, 11002, 3998, 5473], [12890], [15792, 15218], [], [10739, 18605, 10156, 2193, 7157], [], [], [11090, 13832, 3838, 4552], [16416, 10609, 10200, 5737, 963], [], [14827], [12099], [], [18156], [17530, 972], [], [], [4304], [12650, 19022, 4219, 756], [11955, 18977, 11242, 6787, 16946], [14075], [], [], [], [], [17297], [12706, 7053], [6054], [], [10563], [4670, 8874], [6729, 3531, 11855, 11257, 4515], [], [19065, 11881, 11068], [6816, 3294, 7469], [889], [14498], [17530, 11955, 159, 10175], [8234, 13871], [9239, 13664, 8689, 5922, 1699], [], [1360, 14231], [], [15218, 3294], [15591], [18583, 6846, 1240], [16064], [], [9625], [16803, 18977], [10254, 18047, 11708], [8924, 15737], [], [6911, 16161, 8161], [], [403, 5900, 6136, 7706], [], [19214, 7643], [5133, 5909], [12418, 17470, 19061], [16096, 15332, 16394], [12384, 93], [6816, 1467, 3294], [], [465], [12167, 8172, 15547], [13137], [], [5191, 1514, 17470], [1585, 13178], [], [3798], [11058], [], [19313, 9988, 6574], [15675], [16155, 13833, 9524, 4803, 19162], [5997, 15775, 8609, 937, 8897], [16899, 16752, 12213, 14070], [], [6460, 3092], [], []]\n",
            "Dev:  [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
          ]
        }
      ],
      "source": [
        "print(\"Train: \", train_claim_evidences)\n",
        "print(\"Dev: \", dev_claim_evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL0R-_jaCJfv",
        "outputId": "8d6c6be2-fd20-4082-b8a3-1e2fae025416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train claim count:  1228\n",
            "Dev claim count:  154\n",
            "Test claim count:  153\n",
            "Evidence count:  19324\n"
          ]
        }
      ],
      "source": [
        "print(\"Train claim count: \",len(train_claim_text))\n",
        "print(\"Dev claim count: \",len(dev_claim_text))\n",
        "print(\"Test claim count: \",len(test_claim_text))\n",
        "print(\"Evidence count: \",len(cleaned_evidence_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3LL9-aEb1g5"
      },
      "source": [
        "## 1.3 Development Set Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4cbdtTb1g5"
      },
      "source": [
        "In this section, we perform the main tasks of the project on the development set:\n",
        "\n",
        "1. **Evidence Retrieval**: For each claim, find the most relevant evidence from the corpus.\n",
        "2. **Claim Classification**: Predict the label for each claim based on the retrieved evidence and the claim's similarity to the training claims.\n",
        "\n",
        "The code uses TF-IDF vectorization and cosine similarity to measure the relevance between claims and evidence, and between development and training claims. The most similar evidence and training claims are used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Creating two vectorizer\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(max_features=22845, use_idf=True)\n",
        "\n",
        "# fit the vectorizer on the evidence data\n",
        "evidence_tfidf_vectorizer.fit(cleaned_evidence_text)\n",
        "\n",
        "# Transform cleaned_evidence_text\n",
        "transformed_evidence = evidence_tfidf_vectorizer.transform(cleaned_evidence_text)\n",
        "\n",
        "# Transform claim data\n",
        "train_claim_tfidf = evidence_tfidf_vectorizer.transform(train_claim_text)\n",
        "dev_claim_tfidf = evidence_tfidf_vectorizer.transform(dev_claim_text)\n",
        "test_claim_tfidf = evidence_tfidf_vectorizer.transform(test_claim_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed evidence shape:  (19324, 22845)\n",
            "Transformed train claim shape:  (1228, 22845)\n",
            "Transformed dev claim shape:  (154, 22845)\n"
          ]
        }
      ],
      "source": [
        "print(\"Transformed evidence shape: \", transformed_evidence.shape)\n",
        "print(\"Transformed train claim shape: \", train_claim_tfidf.shape)\n",
        "print(\"Transformed dev claim shape: \", dev_claim_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "58n5gcslrAmV"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between train claims and evidence\n",
        "train_similarity = cosine_similarity(train_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between dev claims and evidence\n",
        "dev_similarity = cosine_similarity(dev_claim_tfidf, transformed_evidence)\n",
        "\n",
        "# Calculate cosine similarity between test claims and evidence\n",
        "test_similarity = cosine_similarity(test_claim_tfidf, transformed_evidence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train similarity: [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.07193263 0.        ]\n",
            " [0.         0.         0.         ... 0.10081634 0.09094364 0.        ]\n",
            " ...\n",
            " [0.         0.         0.0274485  ... 0.         0.         0.        ]\n",
            " [0.         0.         0.04372735 ... 0.         0.07984955 0.03913389]\n",
            " [0.         0.         0.01689673 ... 0.         0.         0.        ]]\n",
            "Test similarity:  [[0.         0.         0.06279525 ... 0.         0.         0.        ]\n",
            " [0.07206637 0.         0.04409426 ... 0.         0.06038967 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.09004394 0.         0.02754698 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.0306085  ... 0.         0.         0.        ]\n",
            " [0.         0.         0.04236803 ... 0.04138184 0.08525478 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Train similarity:\", train_similarity)\n",
        "print(\"Test similarity: \", test_similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ErcyOigvPc6k",
        "outputId": "813319b1-0d8a-4c6d-f7ed-6e93574f5f6a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-3a6a14ff729e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_claim_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_evidence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_claim_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_evidence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate Euclidean distance between dev claims and evidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # Calculate Euclidean distance between train claims and evidence\n",
        "# train_distance = np.zeros((train_claim_tfidf.shape[0], transformed_evidence.shape[0]))\n",
        "# for i in range(train_claim_tfidf.shape[0]):\n",
        "#     for j in range(transformed_evidence.shape[0]):\n",
        "#         train_distance[i, j] = euclidean(train_claim_tfidf[i].toarray().ravel(), transformed_evidence[j].toarray().ravel())\n",
        "\n",
        "# # Calculate Euclidean distance between dev claims and evidence\n",
        "# dev_distance = np.zeros((dev_claim_tfidf.shape[0], transformed_evidence.shape[0]))\n",
        "# for i in range(dev_claim_tfidf.shape[0]):\n",
        "#     for j in range(transformed_evidence.shape[0]):\n",
        "#         dev_distance[i, j] = euclidean(dev_claim_tfidf[i].toarray().ravel(), transformed_evidence[j].toarray().ravel())\n",
        "\n",
        "# # Calculate Euclidean distance between test claims and evidence\n",
        "# test_distance = np.zeros((test_claim_tfidf.shape[0], transformed_evidence.shape[0]))\n",
        "# for i in range(test_claim_tfidf.shape[0]):\n",
        "#     for j in range(transformed_evidence.shape[0]):\n",
        "#         test_distance[i, j] = euclidean(test_claim_tfidf[i].toarray().ravel(), transformed_evidence[j].toarray().ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fzgteUN7rAmX"
      },
      "outputs": [],
      "source": [
        "def spliting_dataset(similarity, claim_texts, claim_evidences, evidence_texts, top_k=5, neg_ratio=1):\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Based on the similarity matrix, find the top k most similar evidence for each claim\n",
        "    for i in range(similarity.shape[0]):\n",
        "\n",
        "        claim_text = claim_texts[i]\n",
        "\n",
        "        # Find the top k most similar evidence\n",
        "        top_evidences = np.argsort(-similarity[i])[:top_k]\n",
        "\n",
        "        # Add the top k most similar evidence to the dataset, label as 1\n",
        "        for evidence_index in top_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(1)\n",
        "\n",
        "        # If the claim has evidences, add the evidence to the dataset, label as 1\n",
        "        if claim_evidences is not None:\n",
        "            for evidence_index in claim_evidences[i]:\n",
        "                evidence_text = evidence_texts[evidence_index]\n",
        "                dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "                labels.append(1)\n",
        "\n",
        "        # Randomly sample negative samples, label as 0\n",
        "        neg_samples_num = int(neg_ratio * len(top_evidences))\n",
        "\n",
        "        # Randomly sample negative samples that are not in the top k most similar evidence\n",
        "        neg_evidences = np.random.choice(\n",
        "            [j for j in range(similarity.shape[1]) if j not in top_evidences],\n",
        "            neg_samples_num\n",
        "        )\n",
        "\n",
        "        # Add the negative samples to the dataset\n",
        "        for evidence_index in neg_evidences:\n",
        "            evidence_text = evidence_texts[evidence_index]\n",
        "            dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "            labels.append(0)\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1yZ4FGlrrAmX"
      },
      "outputs": [],
      "source": [
        "train_dataset, train_dataset_labels = spliting_dataset(\n",
        "    train_similarity, train_claim_text, train_claim_evidences, cleaned_evidence_text, top_k=6, neg_ratio=1\n",
        ")\n",
        "dev_dataset, dev_dataset_labels = spliting_dataset(\n",
        "    dev_similarity, dev_claim_text, None, cleaned_evidence_text, top_k=6, neg_ratio=1\n",
        ")\n",
        "test_dataset, test_dataset_labels = spliting_dataset(\n",
        "    test_similarity, test_claim_text, None, cleaned_evidence_text, top_k=6, neg_ratio=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def spliting_dataset(similarity, claim_texts, claim_evidences, evidence_texts, threshold=0.5, neg_ratio=1):\n",
        "#     dataset = []\n",
        "#     labels = []\n",
        "    \n",
        "#     # Calculate the mean similarity for each claim\n",
        "#     mean_similarities = np.mean(similarity, axis=1)\n",
        "    \n",
        "#     # Based on the similarity matrix, find the evidence that has similarity higher than the threshold\n",
        "#     for i in range(similarity.shape[0]):\n",
        "#         claim_text = claim_texts[i]\n",
        "        \n",
        "#         # Find the evidence with similarity higher than the threshold\n",
        "#         top_evidences = np.where(similarity[i] > threshold * mean_similarities[i])[0]\n",
        "        \n",
        "#         # Add the top evidence to the dataset, label as 1\n",
        "#         for evidence_index in top_evidences:\n",
        "#             evidence_text = evidence_texts[evidence_index]\n",
        "#             dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "#             labels.append(1)\n",
        "        \n",
        "#         # If the claim has evidences, add the evidence to the dataset, label as 1\n",
        "#         if claim_evidences is not None:\n",
        "#             for evidence_index in claim_evidences[i]:\n",
        "#                 evidence_text = evidence_texts[evidence_index]\n",
        "#                 dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "#                 labels.append(1)\n",
        "        \n",
        "#         # Randomly sample negative samples, label as 0\n",
        "#         neg_samples_num = int(neg_ratio * len(top_evidences))\n",
        "        \n",
        "#         # Randomly sample negative samples that are not in the top evidence\n",
        "#         neg_evidences = np.random.choice(\n",
        "#             [j for j in range(similarity.shape[1]) if j not in top_evidences], \n",
        "#             neg_samples_num\n",
        "#         )\n",
        "        \n",
        "#         # Add the negative samples to the dataset\n",
        "#         for evidence_index in neg_evidences:\n",
        "#             evidence_text = evidence_texts[evidence_index]\n",
        "#             dataset.append(\"[cls] \" + claim_text + \" [sep] \" + evidence_text)\n",
        "#             labels.append(0)\n",
        "    \n",
        "#     return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_dataset, train_dataset_labels = spliting_dataset(\n",
        "#     train_similarity, train_claim_text, train_claim_evidences, cleaned_evidence_text, threshold=0.8, neg_ratio=1.1\n",
        "# )\n",
        "# dev_dataset, dev_dataset_labels = spliting_dataset(\n",
        "#     dev_similarity, dev_claim_text, None, cleaned_evidence_text, threshold=0.8, neg_ratio=1.1\n",
        "# )\n",
        "# test_dataset, test_dataset_labels = spliting_dataset(\n",
        "#     test_similarity, test_claim_text, None, cleaned_evidence_text, threshold=0.8, neg_ratio=1.1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZxkXsHUgb1g7"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset labels to numpy array\n",
        "train_label_array = np.array(train_dataset_labels)\n",
        "dev_label_array = np.array(dev_dataset_labels)\n",
        "test_label_array = np.array(test_dataset_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a9pkJPuab1g8"
      },
      "outputs": [],
      "source": [
        "# need to install\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ICh3nDmFb1g8"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  # 0 is padding token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W2Sj6qFub1g8"
      },
      "outputs": [],
      "source": [
        "# Convert the text data to sequence\n",
        "train_sequence = tokenizer.texts_to_sequences(train_dataset)\n",
        "dev_sequence = tokenizer.texts_to_sequences(dev_dataset)\n",
        "test_sequence = tokenizer.texts_to_sequences(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KCi9XwMPb1g8"
      },
      "outputs": [],
      "source": [
        "longest_train_sequence = 0\n",
        "for i in train_sequence:\n",
        "    longest_train_sequence = max(longest_train_sequence, len(i))\n",
        "\n",
        "longest_dev_sequence = 0\n",
        "for i in dev_sequence:\n",
        "    longest_dev_sequence = max(longest_dev_sequence, len(i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest train sequence:  238\n",
            "Longest dev sequence:  93\n"
          ]
        }
      ],
      "source": [
        "print(\"Longest train sequence: \", longest_train_sequence)\n",
        "print(\"Longest dev sequence: \", longest_dev_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WQyglIFSb1hA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_length = max(longest_train_sequence, longest_dev_sequence) + 1\n",
        "\n",
        "padded_train_sequence = pad_sequences(train_sequence, maxlen=padding_length, padding='post')\n",
        "padded_dev_sequence = pad_sequences(dev_sequence, maxlen=padding_length, padding='post')\n",
        "padded_test_sequence = pad_sequences(test_sequence, maxlen=padding_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG_0JSzdb1hA",
        "outputId": "1c06eef2-a8ef-4473-f903-4c7110795bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"retrieval_cls_lstm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 239, 60)           942120    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 239, 60)           0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 239, 200)         128800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 200)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,091,121\n",
            "Trainable params: 1,091,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# from workshop\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "#model definition\n",
        "# feedforward network (MLP)\n",
        "model = Sequential(name=\"retrieval_cls_lstm\")\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=padding_length, embeddings_regularizer=l2(0.02)))\n",
        "\n",
        "model.add(layers.Dropout(0.5))\n",
        "# model.add(LSTM(hidden_dim, return_sequences=True, dropout=0.1))\n",
        "# model.add(LSTM(hidden_dim, dropout=0.1))\n",
        "\n",
        "model.add(layers.Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.02), recurrent_regularizer=l2(0.02))))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(layers.Dense(hidden_dim, activation='tanh', kernel_regularizer=l2(0.02), bias_regularizer=l2(0.02)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.Recall()])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "decay_steps = 3000\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    learning_rate, decay_steps\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2RL1LOb1hA",
        "outputId": "46009bae-9fcd-44cb-aef4-0542b33e31c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34/34 [==============================] - 5s 80ms/step - loss: 14.1299 - val_loss: 5.7147\n",
            "Epoch 2/15\n",
            "34/34 [==============================] - 2s 65ms/step - loss: 3.5149 - val_loss: 2.1054\n",
            "Epoch 3/15\n",
            "34/34 [==============================] - 2s 68ms/step - loss: 1.5630 - val_loss: 1.1600\n",
            "Epoch 4/15\n",
            "34/34 [==============================] - 2s 66ms/step - loss: 0.9699 - val_loss: 0.8430\n",
            "Epoch 5/15\n",
            "34/34 [==============================] - 2s 62ms/step - loss: 0.7747 - val_loss: 0.7428\n",
            "Epoch 6/15\n",
            "34/34 [==============================] - 2s 60ms/step - loss: 0.7135 - val_loss: 0.7126\n",
            "Epoch 7/15\n",
            "34/34 [==============================] - 2s 60ms/step - loss: 0.6950 - val_loss: 0.7030\n",
            "Epoch 8/15\n",
            "34/34 [==============================] - 2s 69ms/step - loss: 0.6895 - val_loss: 0.7000\n",
            "Epoch 9/15\n",
            "34/34 [==============================] - 2s 69ms/step - loss: 0.6880 - val_loss: 0.6997\n",
            "Epoch 10/15\n",
            "34/34 [==============================] - 2s 66ms/step - loss: 0.6877 - val_loss: 0.6993\n",
            "Epoch 11/15\n",
            "34/34 [==============================] - 2s 67ms/step - loss: 0.6875 - val_loss: 0.6979\n",
            "Epoch 12/15\n",
            "34/34 [==============================] - 2s 68ms/step - loss: 0.6874 - val_loss: 0.7002\n",
            "Epoch 13/15\n",
            "34/34 [==============================] - 2s 67ms/step - loss: 0.6875 - val_loss: 0.6996\n",
            "Epoch 14/15\n",
            "34/34 [==============================] - 2s 68ms/step - loss: 0.6876 - val_loss: 0.6975\n",
            "Epoch 15/15\n",
            "34/34 [==============================] - 2s 69ms/step - loss: 0.6874 - val_loss: 0.6992\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x216466bcc70>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(padded_train_sequence,train_label_array,epochs=15,validation_data=(padded_dev_sequence, dev_label_array),verbose=True,batch_size=500,callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "cUG6D3SPrAmZ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "# model.save('retrieval_cls_lstm')\n",
        "\n",
        "# Load the model\n",
        "# model = tf.keras.models.load_model('retrieval_cls_lstm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AocxDzmKCJfx",
        "outputId": "67332018-710e-4d55-fd50-0825f465ada0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/29 [==============================] - 1s 19ms/step\n",
            "29/29 [==============================] - 0s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "# Start prediction\n",
        "\n",
        "dev_predictions = model.predict(padded_dev_sequence, batch_size=64)\n",
        "test_predictions = model.predict(padded_test_sequence, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA8lLFsWEnJ_",
        "outputId": "697d9989-e28c-4a05-b877-6c38c6f93814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]]\n",
            "[[0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]\n",
            " [0.55367523]]\n"
          ]
        }
      ],
      "source": [
        "print(dev_predictions[:20])\n",
        "print(test_predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rvtdePsbCJfx"
      },
      "outputs": [],
      "source": [
        "def evidences_retrieval(claim_evidence_scores, top_k):\n",
        "\n",
        "    top_evidence_indices = []\n",
        "\n",
        "    for scores in claim_evidence_scores:\n",
        "        sorted_indices = np.argsort(scores)[::-1]\n",
        "        top_indices = sorted_indices[:top_k]\n",
        "        top_evidence_indices.append(top_indices)\n",
        "\n",
        "    return top_evidence_indices\n",
        "\n",
        "\n",
        "select_evidence_k = 6\n",
        "dev_top_evidence_indices = evidences_retrieval(dev_predictions, select_evidence_k)\n",
        "test_top_evidence_indices = evidences_retrieval(test_predictions, select_evidence_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nmalyxnRCJfx"
      },
      "outputs": [],
      "source": [
        "# Update the dev JSON file\n",
        "with open('data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)\n",
        "\n",
        "for claim_id, evidence_indices in zip(dev_claim_id, dev_top_evidence_indices):\n",
        "    top_evidence_ids = [cleaned_evidence_id[idx] for idx in evidence_indices]\n",
        "    dev_claims[claim_id]['evidences'] = top_evidence_ids\n",
        "\n",
        "with open('data/dev-claims.json', 'w') as f:\n",
        "    json.dump(dev_claims, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Zdpct1P9CJfx"
      },
      "outputs": [],
      "source": [
        "# Update the test JSON file\n",
        "with open('data/test-claims-unlabelled.json', 'r') as f:\n",
        "    test_claims = json.load(f)\n",
        "\n",
        "for claim_id, evidence_indices in zip(test_claim_id, test_top_evidence_indices):\n",
        "    top_evidence_ids = [cleaned_evidence_id[idx] for idx in evidence_indices]\n",
        "    test_claims[claim_id]['evidences'] = top_evidence_ids\n",
        "\n",
        "with open('data/test-claims-unlabelled.json', 'w') as f:\n",
        "    json.dump(test_claims, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "752a3386-e1ed-4f38-fdd5-ddc90bf80e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F): 0.0\n",
            "Claim Classification Accuracy (A): 0.38961038961038963\n",
            "Harmonic Mean of F and A: 0.0\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "output_str = output.decode('utf-8')\n",
        "\n",
        "# Split the output into lines\n",
        "output_lines = output_str.strip().split('\\n')\n",
        "\n",
        "# Format the output\n",
        "formatted_lines = []\n",
        "for line in output_lines:\n",
        "    metric, value = line.split('=')\n",
        "    metric = metric.strip()\n",
        "    value = value.strip()\n",
        "    formatted_line = f\"{metric}: {value}\"\n",
        "    formatted_lines.append(formatted_line)\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "formatted_output = '\\n'.join(formatted_lines)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
