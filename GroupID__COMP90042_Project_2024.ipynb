{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_RZ3Oy5ylIE"
      },
      "source": [
        "# 0.Setting Colab Method for future model developing\n",
        "Firstly, run the following block to mount the drive to the colab. Then, drag the data folder/**eval.py** to the \"Colab Folder Space\" to ensure the code runs successfully.\n",
        "\n",
        "If data folder updated, attempt to forcibly remount, call `drive.mount(\"/content/drive\", force_remount=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH7j4IHspgB",
        "outputId": "8cdd074d-60b9-493a-9d8e-4f9a18b246fa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import operator\n",
        "from statistics import mean\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Reading and gathering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "evidence count:  1208827\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 163\n",
            "Dev evi outside train evi 328\n"
          ]
        }
      ],
      "source": [
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "\n",
        "#EDA\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "evi_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "\n",
        "for key,value in evi_data.items():\n",
        "    evi_count+=1\n",
        "\n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"evidence count: \",evi_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside += 1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Remove unnecessary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\steph\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['evidence-0', 'evidence-1', 'evidence-2', 'evidence-3', 'evidence-4', 'evidence-5', 'evidence-6', 'evidence-7', 'evidence-8', 'evidence-9']\n"
          ]
        }
      ],
      "source": [
        "print(full_evidence_id[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# remove non-english words\n",
        "def is_pure_english(text):\n",
        "    english_letters = set(string.ascii_letters)\n",
        "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return all(char in english_letters or char.isspace() for char in cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1193821\n",
            "1208827\n",
            "('evidence-0', 'John Bennet Lawes, English entrepreneur and agricultural scientist')\n"
          ]
        }
      ],
      "source": [
        "print(len(set(full_evidence_text)))\n",
        "print(len(full_evidence_text))\n",
        "print(next(iter(evi_data.items())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1114577\n",
            "1208827\n"
          ]
        }
      ],
      "source": [
        "# find the keys of a dictionary given a set of values\n",
        "def remove_non_eng(dictionary):\n",
        "    \n",
        "    # create a dictionary with values as keys and keys as values\n",
        "    eng_evi = {}\n",
        "\n",
        "    # populate the dictionary\n",
        "    for key, value in dictionary.items():\n",
        "        if is_pure_english(value):\n",
        "            eng_evi[key] = value\n",
        "    \n",
        "    return eng_evi\n",
        "\n",
        "# find the keys of the english evidence\n",
        "eng_evi = remove_non_eng(evi_data)\n",
        "print(len(eng_evi))\n",
        "print(len(evi_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "english_evidence = list(eng_evi.values())\n",
        "english_evidence_id = list(eng_evi.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re  # 引入正则表达式\n",
        "\n",
        "# 权威网站 https://www.ucdavis.edu/climate/definitions\n",
        "climate_keywords = [\n",
        "    \"climate\", \"environment\", \"global warming\", \"greenhouse effect\", \"carbon\", \"co2\", \"carbon dioxide\", \n",
        "    \"methane\", \"renewable energy\", \"sustainability\", \"ecology\", \"biodiversity\", \n",
        "    \"fossil fuels\", \"emissions\", \"air quality\", \"ozone\", \"solar energy\", \"wind energy\", \n",
        "    \"climate change\", \"climate crisis\", \"climate adaptation\", \"climate mitigation\", \n",
        "    \"ocean\", \"sea levels\", \"ice melting\", \"deforestation\", \"reforestation\", \"pollution\"\n",
        "]\n",
        "\n",
        "\n",
        "# 支持模糊匹配的函数\n",
        "def contains_climate_keywords(text, keywords):\n",
        "    text = text.lower()  # 将文本转为小写以忽略大小写差异\n",
        "    for keyword in keywords:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", text):  # 使用正则表达式匹配整个词\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 移除与气候科学无关的项\n",
        "def filter_climate_related(dictionary, keywords):\n",
        "    cs_evi = {}\n",
        "    for key, value in dictionary.items():\n",
        "        if contains_climate_keywords(value, keywords):\n",
        "            cs_evi[key] = value\n",
        "    return cs_evi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "cs_evi = filter_climate_related(eng_evi, climate_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1114577\n",
            "19324\n"
          ]
        }
      ],
      "source": [
        "print(len(eng_evi))\n",
        "print(len(cs_evi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dev remove stop word会好一点， test不remove反而好\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "\n",
        "# claim_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "\n",
        "\n",
        "evidence_tfidf_vectorizer.fit(train_claim_text+english_evidence) #english_evidence#\n",
        "train_claim_emb_list = claim_tfidf_vectorizer.fit_transform(train_claim_text)\n",
        "\n",
        "\n",
        "full_evi_emb_list = evidence_tfidf_vectorizer.transform(english_evidence) #english_evidence#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1114577, 479945)\n",
            "  (0, 379647)\t0.34129216009737795\n",
            "  (0, 243628)\t0.5418404201055191\n",
            "  (0, 214354)\t0.23690875712907947\n",
            "  (0, 138633)\t0.3635156297255776\n",
            "  (0, 137916)\t0.2362850989994899\n",
            "  (0, 53862)\t0.48806363421951915\n",
            "  (0, 19522)\t0.32805866743609824\n"
          ]
        }
      ],
      "source": [
        "print(full_evi_emb_list.shape)\n",
        "print(full_evi_emb_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "khk_kR5dnDh4"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "evi_k=2\n",
        "claim_k=1\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    test_out_temp = json.load(input_file)\n",
        "\n",
        "for claim_id,claim_value in test_out_temp.items():\n",
        "    # Task1\n",
        "    # 把test claim转化成vector\n",
        "\n",
        "    test_claim_emb = evidence_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "    evi_sim_dict = {}\n",
        "    # 计算出test claim和所有evidence的相似度\n",
        "    sim = cosine_similarity(test_claim_emb, full_evi_emb_list)[0]\n",
        "\n",
        "    for i in range(len(sim)):\n",
        "        evi_sim_dict[english_evidence_id[i]] = sim[i] #full_evidence_id[i]\n",
        "\n",
        "    # 对evidence根据和claim的相似度排序\n",
        "    s_sim = [(k, v) for k, v in sorted(evi_sim_dict.items(), key=lambda item: item[1],reverse=True)][:evi_k]\n",
        "    sel_sim = [k for k,v in s_sim]\n",
        "    # 把最相似的前k个evidence的id写入到test claim的evidence list\n",
        "    test_out_temp[claim_id][\"evidences\"] = sel_sim\n",
        "\n",
        "# # 遍历每个测试声明\n",
        "# for claim_id, claim_value in test_out_temp.items():\n",
        "#     # 把test claim转化成vector 注意我这两问用了不同的vectorizer，因为有不同的预处理步骤，和应用数据目标\n",
        "#     test_claim_emb = claim_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "\n",
        "#     # 计算出test claim和所有train claim的相似度\n",
        "#     claim_sim_dict = {}\n",
        "#     claim_sim = cosine_similarity(test_claim_emb, train_claim_emb_list)[0]\n",
        "#     for i in range(len(claim_sim)):\n",
        "#         claim_sim_dict[train_claim_id[i]] = claim_sim[i]\n",
        "\n",
        "#     # 取最相似的k个train claim\n",
        "#     most_sim_claims = [(k, v) for k, v in sorted(claim_sim_dict.items(), key=lambda item: item[1],reverse=True)]\n",
        "#     # 我这里用的是k=1只考虑最相似的那一个，label拿出来\n",
        "#     most_sim_claim = max(most_sim_claims, key=operator.itemgetter(1))[0]\n",
        "\n",
        "#     test_out_temp[claim_id][\"claim_label\"] = train_claim_data[most_sim_claim][\"claim_label\"]\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"data/dev_predict.json\", \"w\") as outfile:\n",
        "    json.dump(test_out_temp, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 创建词汇表\n",
        "vectorizer = CountVectorizer(max_features=10000)  # 取最常见的10000个词\n",
        "vectorizer.fit([claim['claim_text'] for claim in train_claim_data.values()])  # 假设train_claim_data是包含文本的列表\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "# 构建索引到词和词到索引的映射\n",
        "index_to_word = {index: word for word, index in vocab.items()}\n",
        "word_to_index = vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 初始化 LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 假设 train_claim_data 是一个包含多个词典的列表，每个词典都有 'claim_text' 和 'claim_label'\n",
        "labels = [claim['claim_label'] for claim in train_claim_data.values()]  # 收集所有标签\n",
        "label_encoder.fit(labels)  # 训练 LabelEncoder\n",
        "\n",
        "# 更新 Dataset 类以使用编码后的标签\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_to_index):\n",
        "        self.texts = [self.text_to_indices(text, word_to_index) for text in texts]\n",
        "        self.labels = torch.tensor(label_encoder.transform(labels), dtype=torch.long)  # 转换并存储为 tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def text_to_indices(text, word_to_index):\n",
        "        return [word_to_index.get(word, 0) for word in text.lower().split()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx], dtype=torch.long), self.labels[idx]\n",
        "\n",
        "# 修改 DataLoader 的 collate_fn 以处理填充\n",
        "def collate_train(batch):\n",
        "    text_list, label_list = zip(*batch)  # 解包 batch 中的文本和标签\n",
        "    text_tensor = pad_sequence([torch.tensor(text, dtype=torch.long) for text in text_list], batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(label_list, dtype=torch.long)\n",
        "    return text_tensor, labels\n",
        "def collate_test(batch):\n",
        "    text_list = [item for item in batch]  # 只处理文本\n",
        "    text_tensor = pad_sequence([torch.tensor(text, dtype=torch.long) for text in text_list], batch_first=True, padding_value=0)\n",
        "    return text_tensor  # 只返回文本张量\n",
        "\n",
        "\n",
        "# 使用 Dataset 和 DataLoader\n",
        "train_dataset = TextDataset(\n",
        "    [claim['claim_text'] for claim in train_claim_data.values()],\n",
        "    [claim['claim_label'] for claim in train_claim_data.values()],\n",
        "    word_to_index\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "print(\"Number of classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_7128\\1931497016.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_tensor = pad_sequence([torch.tensor(text, dtype=torch.long) for text in text_list], batch_first=True, padding_value=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.1568992137908936\n",
            "Epoch 2, Loss: 1.3186403512954712\n",
            "Epoch 3, Loss: 1.3106712102890015\n",
            "Epoch 4, Loss: 1.1011826992034912\n",
            "Epoch 5, Loss: 1.037016749382019\n",
            "Epoch 6, Loss: 1.0799404382705688\n",
            "Epoch 7, Loss: 0.7157043814659119\n",
            "Epoch 8, Loss: 0.4122501611709595\n",
            "Epoch 9, Loss: 0.6383563280105591\n",
            "Epoch 10, Loss: 0.14567218720912933\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "# 确保你的模型结构正确\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# 初始化模型、损失函数和优化器\n",
        "model = TextClassifier(len(word_to_index), 300, 256, num_classes)  # 示例参数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 训练循环\n",
        "model.train()\n",
        "for epoch in range(10):  # 示例周期数\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, word_to_index):\n",
        "        self.texts = [self.text_to_indices(text, word_to_index) for text in texts]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx], dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def text_to_indices(text, word_to_index):\n",
        "        return [word_to_index.get(word, 0) for word in text.lower().split() if word in word_to_index]\n",
        "\n",
        "# 假设 test_out_temp 是一个字典，其中每个条目包含一个 'claim_text'\n",
        "test_texts = [claim['claim_text'] for claim_id, claim in test_out_temp.items()]\n",
        "test_dataset = TextDataset(test_texts, word_to_index)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_7128\\1931497016.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_tensor = pad_sequence([torch.tensor(text, dtype=torch.long) for text in text_list], batch_first=True, padding_value=0)\n"
          ]
        }
      ],
      "source": [
        "# 假设模型预测部分的代码是这样的\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts in test_loader:\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.tolist())\n",
        "\n",
        "# 更新 test_out_temp 以包含预测的标签\n",
        "for (claim_id, claim), prediction in zip(test_out_temp.items(), predictions):\n",
        "    # 修正解包问题\n",
        "    test_out_temp[claim_id][\"claim_label\"] = label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"data/dev_predict.json\", \"w\") as outfile:\n",
        "    json.dump(test_out_temp, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n",
            "1114577\n",
            "(1114577, 479945)\n",
            "0.10499500544162468\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(full_evidence_id))\n",
        "print(len(english_evidence_id))\n",
        "print(full_evi_emb_list.shape)\n",
        "#没写进去 可能是sim没算出来#\n",
        "print(evi_sim_dict[\"evidence-1\"])\n",
        "#(1114577, 479945)\n",
        "#0.0 但是text用english但id用full就会是0.1057....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjj2SKenDh4",
        "outputId": "d380c94d-3adf-4f7e-a751-cf737fe5a546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F): 0.10510204081632651\n",
            "Claim Classification Accuracy (A): 0.3961038961038961\n",
            "Harmonic Mean of F and A: 0.1661246397504032\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "output_str = output.decode('utf-8')\n",
        "\n",
        "# Split the output into lines\n",
        "output_lines = output_str.strip().split('\\n')\n",
        "\n",
        "# Format the output\n",
        "formatted_lines = []\n",
        "for line in output_lines:\n",
        "    metric, value = line.split('=')\n",
        "    metric = metric.strip()\n",
        "    value = value.strip()\n",
        "    formatted_line = f\"{metric}: {value}\"\n",
        "    formatted_lines.append(formatted_line)\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "formatted_output = '\\n'.join(formatted_lines)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
