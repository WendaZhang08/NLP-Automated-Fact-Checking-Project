{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import operator\n",
        "from statistics import mean\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim count:  1228\n",
            "max claim length:  332\n",
            "min claim length:  26\n",
            "mean claim length:  122.95521172638436\n",
            "max evidence count:  5\n",
            "min evidence count:  1\n",
            "mean evidence count:  3.3566775244299674\n",
            "max evidence length:  1979\n",
            "min evidence length:  13\n",
            "mean evidence length:  173.5\n",
            "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
            "Dev evi inside train evi 163\n",
            "Dev evi outside train evi 328\n"
          ]
        }
      ],
      "source": [
        "with open('data/train-claims.json', 'r') as input_file:\n",
        "    train_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in development data (claim)\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    dev_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in test data (claim)\n",
        "with open('data/test-claims-unlabelled.json', 'r') as input_file:\n",
        "    test_claim_data = json.load(input_file)\n",
        "\n",
        "# Read in evidence data\n",
        "with open('data/evidence.json', 'r') as input_file:\n",
        "    evi_data = json.load(input_file)\n",
        "    \n",
        "#EDA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "claim_count = 0\n",
        "claim_length = []\n",
        "evidence_count = []\n",
        "evidence_length = []\n",
        "labels = []\n",
        "\n",
        "for key,value in train_claim_data.items():\n",
        "    claim_count+=1\n",
        "    claim_length.append(len(value[\"claim_text\"]))\n",
        "    evidence_count.append(len(value[\"evidences\"]))\n",
        "    evidence_length += [len(evi_data[x]) for x in value[\"evidences\"]]\n",
        "    labels.append(value[\"claim_label\"])\n",
        "    \n",
        "print(\"claim count: \",claim_count)\n",
        "print(\"max claim length: \",max(claim_length))\n",
        "print(\"min claim length: \",min(claim_length))\n",
        "print(\"mean claim length: \",mean(claim_length))\n",
        "print(\"max evidence count: \",max(evidence_count))\n",
        "print(\"min evidence count: \",min(evidence_count))\n",
        "print(\"mean evidence count: \",mean(evidence_count))\n",
        "print(\"max evidence length: \",max(evidence_length))\n",
        "print(\"min evidence length: \",min(evidence_length))\n",
        "print(\"mean evidence length: \",mean(evidence_length))\n",
        "print(Counter(labels))\n",
        "\n",
        "\n",
        "\n",
        "inside = 0\n",
        "outside = 0\n",
        "\n",
        "train_evi_id = []\n",
        "for claim_id,claim_value in train_claim_data.items():\n",
        "    train_evi_id=train_evi_id+claim_value['evidences']\n",
        "\n",
        "for claim_id,claim_value in dev_claim_data.items():\n",
        "    test_evi_id=claim_value['evidences']\n",
        "    for e in test_evi_id:\n",
        "        if e in train_evi_id:\n",
        "            inside +=1\n",
        "        else:\n",
        "            outside += 1\n",
        "print(\"Dev evi inside train evi\", inside)\n",
        "print(\"Dev evi outside train evi\", outside)\n",
        "\n",
        "full_evidence_id = list(evi_data.keys())\n",
        "full_evidence_text  = list(evi_data.values())\n",
        "train_claim_id = list(train_claim_data.keys())\n",
        "train_claim_text  = [ v[\"claim_text\"] for v in train_claim_data.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dev remove stop word会好一点， test不remove反而好\n",
        "evidence_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "\n",
        "# claim_tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
        "claim_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "\n",
        "\n",
        "evidence_tfidf_vectorizer.fit(train_claim_text+full_evidence_text)\n",
        "train_claim_emb_list = claim_tfidf_vectorizer.fit_transform(train_claim_text)\n",
        "\n",
        "\n",
        "full_evi_emb_list = evidence_tfidf_vectorizer.transform(full_evidence_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "evi_k=2\n",
        "claim_k=1\n",
        "\n",
        "with open('data/dev-claims.json', 'r') as input_file:\n",
        "    test_out_temp = json.load(input_file)\n",
        "\n",
        "for claim_id,claim_value in test_out_temp.items():\n",
        "    # Task1\n",
        "    # 把test claim转化成vector\n",
        "    \n",
        "    test_claim_emb = evidence_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "    evi_sim_dict = {}\n",
        "    # 计算出test claim和所有evidence的相似度\n",
        "    sim = cosine_similarity(test_claim_emb, full_evi_emb_list)[0]\n",
        "    \n",
        "    for i in range(len(sim)):\n",
        "        evi_sim_dict[full_evidence_id[i]] = sim[i]\n",
        "    \n",
        "    # 对evidence根据和claim的相似度排序\n",
        "    s_sim = [(k, v) for k, v in sorted(evi_sim_dict.items(), key=lambda item: item[1],reverse=True)][:evi_k]\n",
        "    sel_sim = [k for k,v in s_sim]\n",
        "    # 把最相似的前k个evidence的id写入到test claim的evidence list\n",
        "    test_out_temp[claim_id][\"evidences\"] = sel_sim\n",
        "   \n",
        "    # Task2\n",
        "    # 把test claim转化成vector 注意我这两问用了不同的vectorizer，因为有不同的预处理步骤，和应用数据目标\n",
        "    test_claim_emb = claim_tfidf_vectorizer.transform([claim_value['claim_text']])\n",
        "\n",
        "    # 计算出test claim和所有train claim的相似度\n",
        "    claim_sim_dict = {}\n",
        "    claim_sim = cosine_similarity(test_claim_emb, train_claim_emb_list)[0]\n",
        "    for i in range(len(claim_sim)):\n",
        "        claim_sim_dict[train_claim_id[i]] = claim_sim[i]\n",
        "    \n",
        "    # 取最相似的k个train claim\n",
        "    most_sim_claims = [(k, v) for k, v in sorted(claim_sim_dict.items(), key=lambda item: item[1],reverse=True)]\n",
        "    # 我这里用的是k=1只考虑最相似的那一个，label拿出来\n",
        "    most_sim_claim = max(most_sim_claims, key=operator.itemgetter(1))[0]\n",
        "    \n",
        "    test_out_temp[claim_id][\"claim_label\"] = train_claim_data[most_sim_claim][\"claim_label\"]\n",
        "\n",
        "\n",
        " \n",
        "# Writing to sample.json\n",
        "with open(\"data/dev_predict.json\", \"w\") as outfile:\n",
        "    json.dump(test_out_temp, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Evidence Retrieval F-score (F)    = 0.10510204081632651\\r\\nClaim Classification Accuracy (A) = 0.474025974025974\\r\\nHarmonic Mean of F and A          = 0.17205555936935077\\r\\n'\n"
          ]
        }
      ],
      "source": [
        "# %%cmd\n",
        "# python eval.py --predictions dev-claims-baseline.json --groundtruth dev-claims.json\n",
        "# python eval.py --predictions dev_predict.json --groundtruth dev-claims.json\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# proc = subprocess.Popen([\"python\", \"eval.py\", \"--predictions\", \"data\\dev_predict.json\", \"--groundtruth\", \"data\\dev-claims.json\"\n",
        "# ], stdout=subprocess.PIPE, shell=True)\n",
        "# (out, err) = proc.communicate()\n",
        "# print(str(out))\n",
        "\n",
        "# 高自动化模型/预处理选择，可以自动读取准确度\n",
        "output = subprocess.check_output(\"python eval.py --predictions data/dev_predict.json --groundtruth data/dev-claims.json\", shell=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
