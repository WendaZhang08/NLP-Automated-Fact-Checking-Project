{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2024 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use keras, tensorflow, nltk, scikit-learn in this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvff21Hv8zjk",
    "tags": []
   },
   "source": [
    "## PreProcess for evidence and claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev_cls_data = json.load(open(\"dev_cls_data.json\", \"r\"))\n",
    "test_cls_data = json.load(open(\"test_cls_data.json\", \"r\"))\n",
    "\n",
    "dev_ids = json.load(open(\"temp_data/dev_ids.json\", \"r\"))\n",
    "test_ids = json.load(open(\"temp_data/test_ids.json\", \"r\"))\n",
    "\n",
    "train_text_idx = json.load(open(\"temp_data/train_text_idx.json\", \"r\"))\n",
    "evidences_text_idx = json.load(open(\"temp_data/evidences_text_idx.json\", \"r\"))\n",
    "\n",
    "text_max_len = 60\n",
    "evidence_max_len = 100\n",
    "all_max_len = 580\n",
    "retrieval_num = 5\n",
    "\n",
    "id2labels = [\"SUPPORTS\", \"NOT_ENOUGH_INFO\", \"REFUTES\", \"DISPUTED\"]\n",
    "labels2id = {\"SUPPORTS\": 0, \"NOT_ENOUGH_INFO\": 1, \"REFUTES\": 2, \"DISPUTED\": 3}\n",
    "\n",
    "train_labels = json.load(open(\"temp_data/train_labels.json\", \"r\"))\n",
    "train_evidences = json.load(open(\"temp_data/train_evidences.json\", \"r\"))\n",
    "\n",
    "idx2word = json.load(open(\"temp_data/idx2word.json\", \"r\"))\n",
    "word2idx = json.load(open(\"temp_data/word2idx.json\", \"r\"))\n",
    "\n",
    "train_negative_evidences = json.load(open(\"pred_train_negative_evidences.json\", \"r\"))\n",
    "evidences_id_dict = json.load(open(\"temp_data/evidences_id_dict.json\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, text_data, evidence_data, positive_evidences, negative_evidences, cls_label, cls_idx, sep_idx, pad_idx, evidence_num=5):\n",
    "        self.text_data = text_data\n",
    "        self.evidence_data = evidence_data\n",
    "        \n",
    "        self.negative_evidences = negative_evidences\n",
    "\n",
    "        self.cls_label = [labels2id[i] for i in cls_label]\n",
    "        self.evidence_num = evidence_num\n",
    "        self.positive_evidences = positive_evidences\n",
    "        \n",
    "        self.cls_idx = cls_idx\n",
    "        self.sep_idx = sep_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.text_data[idx][:text_max_len], self.positive_evidences[idx], self.negative_evidences[idx], self.cls_label[idx]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        queries = []\n",
    "        queries_pos = []\n",
    "        labels = []\n",
    "        \n",
    "        for i, j, h, k in batch:\n",
    "            temp_text = [self.cls_idx]\n",
    "            temp_text.extend(i)\n",
    "            for p in j:\n",
    "                temp_text.append(self.sep_idx)\n",
    "                temp_text.extend(self.evidence_data[p][:evidence_max_len])\n",
    "            if self.evidence_num > len(j):\n",
    "                n = random.sample(h, self.evidence_num - len(j))\n",
    "                for p in n:\n",
    "                    temp_text.append(self.sep_idx)\n",
    "                    temp_text.extend(self.evidence_data[p][:evidence_max_len])\n",
    "            temp_text.append(self.sep_idx)\n",
    "            if len(temp_text) < all_max_len:\n",
    "                temp_text.extend([self.pad_idx] * (all_max_len - len(temp_text)))\n",
    "                \n",
    "            queries.append(temp_text)\n",
    "            queries_pos.append(list(range(all_max_len)))\n",
    "            labels.append(k)    \n",
    "\n",
    "        batch_encoding = {}\n",
    "        batch_encoding[\"queries\"] = torch.LongTensor(queries)        \n",
    "        batch_encoding[\"queries_pos\"] = torch.LongTensor(queries_pos)\n",
    "        batch_encoding[\"labels\"] = torch.LongTensor(labels)\n",
    "        \n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_inputs = [i['text'] for i in dev_cls_data]\n",
    "test_inputs = [i['text'] for i in test_cls_data]\n",
    "dev_outputs = [labels2id[i[\"label\"]] for i in dev_cls_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TrainDataset(train_text_idx, evidences_text_idx, train_evidences, train_negative_evidences, train_labels, word2idx[\"<cls>\"], word2idx[\"<sep>\"], word2idx[\"<pad>\"], evidence_num=retrieval_num)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_set, batch_size=10, shuffle=True, num_workers=4, collate_fn=train_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from workshop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CLS(nn.Module):\n",
    "    def __init__(self, vocab_emb, embed_dim, hidden_size, output_size, nhead, num_layers, max_position=all_max_len):\n",
    "        super(CLS, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_emb, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_position, embed_dim)\n",
    "        \n",
    "        # encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, batch_first=True)\n",
    "        # self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(hidden_size))\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_size, num_layers=2)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.cls = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, text_data, position_text):\n",
    "        mask_ = text_data == 0\n",
    "        # print(text_data.size(), position_text.size())\n",
    "        text_x = self.embedding(text_data) + self.pos_embedding(position_text) * 0.01\n",
    "        #x_encoded = self.encoder(text_x, src_key_padding_mask=mask_)\n",
    "        x_encoded,_ = self.encoder(text_x)\n",
    "        x_cls = x_encoded[:, 0, :]\n",
    "        x_hidden = F.tanh(self.hidden_layer(x_cls))\n",
    "        self.dropout(x_hidden)\n",
    "        cls_res = self.cls(x_hidden)\n",
    "        return cls_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLS(\n",
       "  (embedding): Embedding(86627, 512)\n",
       "  (pos_embedding): Embedding(700, 512)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (hidden_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (cls): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model = CLS(vocab_emb=len(idx2word), embed_dim=512, hidden_size=512, output_size=4, nhead=8, num_layers=6, max_position=700)\n",
    "cls_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from workshop but need to change because I add some speciall setting\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "random.seed(42)\n",
    "\n",
    "encoder_optimizer = optim.Adam(cls_model.parameters())\n",
    "max_lr = 1e-3\n",
    "for param_group in encoder_optimizer.param_groups:\n",
    "    param_group['lr'] = max_lr\n",
    "accumulate_step = 2\n",
    "grad_norm = 4\n",
    "warmup_steps = 300\n",
    "report_freq = 10\n",
    "eval_interval = 50\n",
    "save_dir = \"model_ckpts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dev_input, dev_output, cls_model_):\n",
    "    # get evidence embeddings\n",
    "    start_idx = 0\n",
    "    batch_size = 50\n",
    "    pos_len = len(dev_input[0])\n",
    "    cls_model.eval()\n",
    "\n",
    "    acc = []\n",
    "    correct_count = 0\n",
    "    while start_idx < len(dev_output):\n",
    "        end_idx = min(start_idx + batch_size, len(dev_output))\n",
    "        \n",
    "        cur_input = torch.LongTensor(dev_input[start_idx:end_idx]).view(-1, pos_len).cuda()\n",
    "        cur_pos = torch.LongTensor([list(range(pos_len)) for _ in range(end_idx - start_idx)]).cuda()\n",
    "\n",
    "        cur_res = cls_model_(cur_input, cur_pos)\n",
    "        cur_res = torch.argmax(cur_res, 1).tolist()\n",
    "        \n",
    "        del cur_input, cur_pos\n",
    "        \n",
    "        for i, j in zip(cur_res, dev_output[start_idx: end_idx]):\n",
    "            if i == j:\n",
    "                correct_count += 1\n",
    "        \n",
    "        start_idx = end_idx\n",
    "    acc = correct_count / len(dev_output)\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Accuracy: %.3f\" % acc)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    cls_model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=MelMoxue_NLP_CLS.ipynb\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME MelMoxue_NLP_CLS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbruce\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdbd35bfd6f4066929a88e473674fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112690433381228, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20240425_050130-c0uruoka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bruce/nlp/runs/c0uruoka' target=\"_blank\">cls</a></strong> to <a href='https://wandb.ai/bruce/nlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bruce/nlp' target=\"_blank\">https://wandb.ai/bruce/nlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bruce/nlp/runs/c0uruoka' target=\"_blank\">https://wandb.ai/bruce/nlp/runs/c0uruoka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 22/123 [00:01<00:04, 22.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 10, avg loss: 1.916283\n",
      "learning rate: 0.000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 43/123 [00:02<00:03, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 20, avg loss: 1.431737\n",
      "learning rate: 0.000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 64/123 [00:03<00:02, 23.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 30, avg loss: 1.427802\n",
      "learning rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 82/123 [00:03<00:01, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 40, avg loss: 1.413639\n",
      "learning rate: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 97/123 [00:04<00:01, 23.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 50, avg loss: 1.413316\n",
      "learning rate: 0.000167\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n",
      "\n",
      "\n",
      "\n",
      "Classification Accuracy: 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 103/123 [00:05<00:01, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "best val loss - epoch: 0, epoch_step: 50\n",
      "maximum_f_score 0.11688311688311688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:06<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1, epoch_step: 60, avg loss: 1.445513\n",
      "learning rate: 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 19/123 [00:01<00:04, 22.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 9, avg loss: 1.421385\n",
      "learning rate: 0.000233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 40/123 [00:01<00:03, 23.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 19, avg loss: 1.422743\n",
      "learning rate: 0.000267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 61/123 [00:02<00:02, 23.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 29, avg loss: 1.418498\n",
      "learning rate: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 76/123 [00:03<00:01, 23.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 39, avg loss: 1.416530\n",
      "learning rate: 0.000333\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n",
      "\n",
      "\n",
      "\n",
      "Classification Accuracy: 0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 79/123 [00:04<00:04,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "best val loss - epoch: 1, epoch_step: 39\n",
      "maximum_f_score 0.2662337662337662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 100/123 [00:04<00:01, 21.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 49, avg loss: 1.414500\n",
      "learning rate: 0.000367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 121/123 [00:05<00:00, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2, epoch_step: 59, avg loss: 1.385892\n",
      "learning rate: 0.000400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:06<00:00, 20.47it/s]\n",
      " 13%|█▎        | 16/123 [00:00<00:04, 21.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 7, avg loss: 1.371151\n",
      "learning rate: 0.000433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 37/123 [00:01<00:03, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 17, avg loss: 1.388361\n",
      "learning rate: 0.000467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 52/123 [00:02<00:02, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 27, avg loss: 1.480666\n",
      "learning rate: 0.000500\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n",
      "\n",
      "\n",
      "\n",
      "Classification Accuracy: 0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 76/123 [00:03<00:02, 22.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 37, avg loss: 1.394980\n",
      "learning rate: 0.000533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 97/123 [00:04<00:01, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 47, avg loss: 1.391847\n",
      "learning rate: 0.000567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 118/123 [00:05<00:00, 23.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3, epoch_step: 57, avg loss: 1.401002\n",
      "learning rate: 0.000600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:05<00:00, 21.81it/s]\n",
      " 11%|█         | 13/123 [00:00<00:05, 20.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 6, avg loss: 1.431069\n",
      "learning rate: 0.000633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 28/123 [00:01<00:04, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 16, avg loss: 1.388191\n",
      "learning rate: 0.000667\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 34/123 [00:01<00:05, 17.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classification Accuracy: 0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 55/123 [00:02<00:02, 23.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 26, avg loss: 1.413971\n",
      "learning rate: 0.000700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 73/123 [00:03<00:02, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 36, avg loss: 1.395924\n",
      "learning rate: 0.000733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 94/123 [00:04<00:01, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 46, avg loss: 1.422175\n",
      "learning rate: 0.000767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 115/123 [00:05<00:00, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4, epoch_step: 56, avg loss: 1.428711\n",
      "learning rate: 0.000800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:05<00:00, 21.87it/s]\n",
      "  6%|▌         | 7/123 [00:00<00:07, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 4, avg loss: 1.431863\n",
      "learning rate: 0.000833\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n",
      "\n",
      "\n",
      "\n",
      "Classification Accuracy: 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 30/123 [00:01<00:04, 22.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 14, avg loss: 1.440198\n",
      "learning rate: 0.000867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 51/123 [00:02<00:03, 23.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 24, avg loss: 1.398574\n",
      "learning rate: 0.000900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 72/123 [00:03<00:02, 23.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 34, avg loss: 1.424734\n",
      "learning rate: 0.000933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 90/123 [00:04<00:01, 23.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 44, avg loss: 1.405719\n",
      "learning rate: 0.000967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 105/123 [00:04<00:00, 24.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5, epoch_step: 54, avg loss: 1.447049\n",
      "learning rate: 0.001000\n",
      "\n",
      "\n",
      "\n",
      "Evaluate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 111/123 [00:05<00:00, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classification Accuracy: 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:05<00:00, 21.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "import wandb\n",
    "import os\n",
    "wandb.init(project=\"nlp\", name=\"cls\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "step_cnt = 0\n",
    "all_step_cnt = 0\n",
    "avg_loss = 0\n",
    "maximum_f_score = 0\n",
    "ce_fn = nn.CrossEntropyLoss(torch.FloatTensor([0.2, 0.3, 0.5, 1.]).cuda())\n",
    "\n",
    "for epoch in range(5): \n",
    "    epoch_step = 0\n",
    "\n",
    "    for (i, batch) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        step_cnt += 1\n",
    "        \n",
    "        # forward pass\n",
    "            \n",
    "        cur_res = cls_model(batch[\"queries\"].cuda(), batch[\"queries_pos\"].cuda())\n",
    "\n",
    "        loss = ce_fn(cur_res, batch[\"labels\"].cuda())\n",
    "        loss = loss / accumulate_step\n",
    "        loss.backward()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        if step_cnt == accumulate_step:\n",
    "            # updating\n",
    "            if grad_norm > 0:\n",
    "                nn.utils.clip_grad_norm_(cls_model.parameters(), grad_norm)\n",
    "\n",
    "            step_cnt = 0\n",
    "            epoch_step += 1\n",
    "            all_step_cnt += 1\n",
    "            \n",
    "            # adjust learning rate\n",
    "            if all_step_cnt <= warmup_steps:\n",
    "                lr = all_step_cnt * (max_lr - 2e-8) / warmup_steps + 2e-8\n",
    "            else:\n",
    "                lr = max_lr - (all_step_cnt - warmup_steps) * 1e-6\n",
    "                \n",
    "            encoder_optimizer.step()\n",
    "            encoder_optimizer.zero_grad()\n",
    "        \n",
    "        if all_step_cnt % report_freq == 0 and step_cnt == 0:\n",
    "            if all_step_cnt <= warmup_steps:\n",
    "                lr = all_step_cnt * (max_lr - 2e-8) / warmup_steps + 2e-8\n",
    "            else:\n",
    "                lr = max_lr - (all_step_cnt - warmup_steps) * 1e-6\n",
    "\n",
    "            wandb.log({\"learning_rate\": lr}, step=all_step_cnt)\n",
    "            wandb.log({\"loss\": avg_loss / report_freq}, step=all_step_cnt)\n",
    "            \n",
    "            # report stats\n",
    "            print(\"\\n\")\n",
    "            print(\"epoch: %d, epoch_step: %d, avg loss: %.6f\" % (epoch + 1, epoch_step, avg_loss / report_freq))\n",
    "            print(f\"learning rate: {lr:.6f}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            avg_loss = 0\n",
    "        del loss, cur_res\n",
    "\n",
    "        if all_step_cnt % eval_interval == 0 and all_step_cnt != 0 and step_cnt == 0:\n",
    "            # evaluate the model as a scorer\n",
    "            print(\"\\nEvaluate:\\n\")\n",
    "            \n",
    "            f_score = validate(dev_inputs, dev_outputs, cls_model)\n",
    "            wandb.log({\"acc\": f_score}, step=all_step_cnt)\n",
    "\n",
    "            if f_score > maximum_f_score:\n",
    "                maximum_f_score = f_score\n",
    "                torch.save(cls_model.state_dict(), os.path.join(save_dir, \"best_cls_ckpt.bin\"))\n",
    "                # torch.save(last_evidence_embeddings, os.path.join(save_dir, \"evidence_embeddings\"))\n",
    "                print(\"\\n\")\n",
    "                print(\"best val loss - epoch: %d, epoch_step: %d\" % (epoch, epoch_step))\n",
    "                print(\"maximum_f_score\", f_score)\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dev_input, cls_model_):\n",
    "    # get evidence embeddings\n",
    "    start_idx = 0\n",
    "    batch_size = 50\n",
    "    pos_len = len(dev_input[0])\n",
    "    cls_model.eval()\n",
    "\n",
    "    cls_res = []\n",
    "    correct_count = 0\n",
    "    while start_idx < len(dev_input):\n",
    "        end_idx = min(start_idx + batch_size, len(dev_input))\n",
    "        \n",
    "        cur_input = torch.LongTensor(dev_input[start_idx:end_idx]).view(-1, pos_len).cuda()\n",
    "        cur_pos = torch.LongTensor([list(range(pos_len)) for _ in range(end_idx - start_idx)]).cuda()\n",
    "\n",
    "        cur_res = cls_model_(cur_input, cur_pos)\n",
    "        cur_res = torch.argmax(cur_res, 1).tolist()\n",
    "        \n",
    "        del cur_input, cur_pos\n",
    "        \n",
    "        cls_res.extend(cur_res)\n",
    "        \n",
    "        start_idx = end_idx\n",
    "\n",
    "    return cls_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cls_model.load_state_dict(torch.load(os.path.join(save_dir, \"best_cls_ckpt.bin\")))\n",
    "\n",
    "dev_classes = predict(dev_inputs, cls_model)\n",
    "test_classes = predict(test_inputs, cls_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev_claims = json.load(open(\"pred_dev_claims_retrieval.json\", \"r\"))\n",
    "pred_test_claims = json.load(open(\"pred_test_claims_retrieval.json\", \"r\"))\n",
    "\n",
    "for i, j in zip(dev_ids, dev_classes):\n",
    "    pred_dev_claims[i]['claim_label'] = id2labels[j]\n",
    "\n",
    "for i, j in zip(test_ids, test_classes):\n",
    "    pred_test_claims[i]['claim_label'] = id2labels[j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save cls data\n",
    "json.dump(pred_dev_claims, open(\"pred_dev_claims.json\", \"w\"))\n",
    "json.dump(pred_test_claims, open(\"pred_test_claims.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 154})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(dev_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 153})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP",
    "tags": []
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python eval.py --predictions pred_dev_claims.json --groundtruth data/dev-claims.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
